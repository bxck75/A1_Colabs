{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxck75/A1_Colabs/blob/master/Version4_uLtImAtEsTaRtEr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installs"
      ],
      "metadata": {
        "id": "O6G-ONPfb3re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,subprocess\n",
        "\n",
        "# system functions\n",
        "def recursive_glob(treeroot, pattern):\n",
        "  results = []\n",
        "  for base, dirs, files in os.walk(treeroot):\n",
        "    goodfiles = fnmatch.filter(files, pattern)\n",
        "    results.extend(os.path.join(base, f) for f in goodfiles)\n",
        "  return results\n",
        "\n",
        "def cmdline(cmd,pr=False):\n",
        "  result =[]\n",
        "  f = os.popen(cmd)\n",
        "  try:\n",
        "    for line in f:\n",
        "      if pr == True:\n",
        "        print(line)\n",
        "        result.append(line),\n",
        "  finally:\n",
        "    f.close()\n",
        "  return result\n",
        "\n",
        "# stinking default folder...poef!\n",
        "cmdline('rm -r /content/sample_data')\n",
        "cmdline('pip install gitpython')\n",
        "# pick fs\n",
        "filesystem = \"local\" #@param [\"gdrive\", \"local\"]\n",
        "if filesystem == 'gdrive':\n",
        "  # sync google drive\n",
        "  from google.colab import drive\n",
        "  import os\n",
        "  # if drive is needed uncomment\n",
        "  print('Google drive ',end='')\n",
        "  drive.mount('/content/drive',force_remount=True)\n",
        "  print('Root folder set to ',end='')\n",
        "  root = '/content/drive/My drive/image_learning'\n",
        "  print(root)\n",
        "else:\n",
        "  print('Root folder set to ',end='')\n",
        "  root = '/content/image_learning'\n",
        "  print(root) \n",
        "\n",
        "#shoot root\n",
        "os.makedirs(root,exist_ok = True)\n",
        "os.chdir(root)\n",
        "\n",
        "# install missing lib\n",
        "cmdline('pip install theano sklearn imageio tensorflow-gpu==2.0.0-alpha0')\n",
        "cmdline('apt install tree')\n"
      ],
      "metadata": {
        "id": "1c3SyHHEb280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa00a7b-eb84-462c-ca25-cbfa2b195845"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root folder set to /content/image_learning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "zdtL2r2KcGJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import the rest\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D,MaxPooling2D\n",
        "from keras.optimizers import SGD, RMSprop,adam\n",
        "from keras.utils import np_utils\n",
        "import  glob, shutil, fnmatch, time, cv2, itertools\n",
        "from IPython.display import clear_output, display, Image\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import ipywidgets as widgets\n",
        "from subprocess import PIPE, Popen\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Pu_Y1BqDcEzE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GitGo Class"
      ],
      "metadata": {
        "id": "GHQHd0VZce3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of reps uncomment to install\n",
        "reps=[\n",
        "#         'EIGENREPS',\n",
        "        'ytdl-org/youtube-dl',\n",
        "        'mikf/gallery-dl',\n",
        "        'bxck75/piss-ant-pix2pix',\n",
        "        'bxck75/A1_CycleGAN-and-pix2pix_with_colab',\n",
        "#        'bxck75/CartoonGAN-Test-Pytorch-Torch',\n",
        "        'eriklindernoren/Keras-GAN',\n",
        "#         'mikf/gallery-dl',\n",
        "#         'bxck75/datasets',\n",
        "#         'EIGENREPS',\n",
        "#         'tjwei/Flappy-Turtle.',\n",
        "#         'tjwei/fonttools',\n",
        "#         'tjwei/blender3d_import_psk_psa',\n",
        "#         'lllyasviel/sketchKeras',\n",
        "#         'Mckinsey666/Anime-Face-Dataset',\n",
        "#         'chenyuntc/pytorch-book',\n",
        "#         'lllyasviel/style2paints',\n",
        "#         'llSourcell/GANS-for-style-transfer',\n",
        "#         'opencv/open_model_zoo',\n",
        "         'hindupuravinash/the-gan-zoo',\n",
        "#         'corenel/GAN-Zoo',\n",
        "#         'eriklindernoren/Keras-GAN',\n",
        "#        'junyanz/CycleGAN',\n",
        "#         'junyanz/pytorch-CycleGAN-and-pix2pix',\n",
        "#         'junyanz/iGAN', #----> !wget http://efrosgans.eecs.berkeley.edu/iGAN/datasets/church_64.zip <----dataset \toutdoor_128.zip \thandbag_128.zip !!!\n",
        "#         'martinarjovsky/WassersteinGAN',\n",
        "#         'shaoanlu/faceswap-GAN',\n",
        "#           'LantaoYu/SeqGAN',\n",
        "#         'tjwei/GANotebooks',\n",
        "#         'adeshpande3/Tensorflow-Programs-and-Tutorials',\n",
        "#         'adeshpande3/Generative-Adversarial-Networks',\n",
        "#         'diegoalejogm/gans',\n",
        "#         'osh/KerasGAN',\n",
        "#         'r9y9/gantts',\n",
        "#         'jayleicn/animeGAN',\n",
        "#         'jayleicn/ImageNet-Training',\n",
        "#         'Zardinality/WGAN-tensorflow',\n",
        "#         'timsainb/Tensorflow-MultiGPU-VAE-GAN',\n",
        "#         'Larox/python-moviepy-meetup',\n",
        "#         'tjwei/keras-yolo3',\n",
        "#         'tensorflow/gan',\n",
        "#         'tensorflow/moonlight'\n",
        "#         'tensorflow/models',\n",
        "#         'tensorflow/datasets',\n",
        "#         'tensorflow/docs',\n",
        "#         'mnicnc404/CartoonGan-tensorflow',\n",
        "#         'Yijunmaverick/CartoonGAN-Test-Pytorch-Torch',\n",
        "#         'keras-team/keras-contrib',\n",
        "#         'mnicnc404/CartoonGan-tensorflow',\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Gitgo class\n",
        "class GitGo():\n",
        "  \n",
        "  def __init__(self,repos,sub_repos=False,chdir=True,path='/content/'):\n",
        "    self.sub_repo_list = []\n",
        "    self.GitUsers=[]\n",
        "    self.repos = repos\n",
        "    self.chdir = chdir\n",
        "    self.path = path\n",
        "    os.makedirs(self.path, exist_ok = True)\n",
        "    if 'help' in self.repos:\n",
        "      self.help()\n",
        "    self.install_reps()\n",
        "    self.custom_reps_setup()\n",
        "    if sub_repos == True:\n",
        "      self.get_other_reps()\n",
        "\n",
        "  def help(self):\n",
        "    return \"* pulls git rep and shows files \\\n",
        "            * returns root path for the repository \\\n",
        "            * Function needs repository <user>/<repository name> combination\\\n",
        "            * Switch chdir and define the rootpath for the repository\\\n",
        "            * Use : GitGo(<list of reps to install>, sub_repos=<True/False, chdir=<True/False>, path=<root path>)\\\n",
        "            \"\n",
        "  \n",
        "  def install_reps(self):    \n",
        "    for rep in self.repos:\n",
        "      self.rep=rep.split('/')\n",
        "      # change folder check\n",
        "      if self.chdir ==True:\n",
        "        #Switch to path\n",
        "        os.chdir(self.path)\n",
        "      # pull the git repo\n",
        "      cmdline('git clone https://github.com/'+self.rep[0]+'/'+self.rep[1]+'.git',True)\n",
        "      # Set the return value for rep rootpath\n",
        "      self.PATH=self.path+self.rep[1]\n",
        "    # show imported files\n",
        "    os.system('ls ' + root)\n",
        "\n",
        "  def custom_reps_setup(self):\n",
        "    # custom stuff for CartoonGAN-tensorflow and keras-team/keras-contrib\n",
        "    if 'keras-team/keras-contrib' in self.repos:\n",
        "      os.chdir(self.path+'/keras-contrib')\n",
        "      cmdline('python convert_to_tf_keras.py')\n",
        "      cmdline('USE_TF_KERAS=1')\n",
        "      cmdline('python setup.py install')\n",
        "      import tensorflow as tf\n",
        "      tf.__version__     \n",
        "    # custom setup stuff for gallery-dl repo\n",
        "    if 'mikf/gallery-dl' in self.repos:\n",
        "      os.chdir(root+'/gallery-dl')\n",
        "      cmdline(\"pip install -e . |grep 'succes'\",True)\n",
        "    # custom setup stuff for youtube-dl repo\n",
        "    if 'ytdl-org/youtube-dl' in self.repos:\n",
        "      os.chdir(root+'/youtube-dl') \n",
        "      cmdline(\"pip install -e . |grep 'succes'\",True)      \n",
        "    # switch backt to root\n",
        "    os.chdir(self.path)\n",
        "         \n",
        "  def get_other_reps(self):          \n",
        "      for r in self.repos:\n",
        "        self.GUSER=r.split('/')[0]\n",
        "        self.repo_name=r.split('/')[1]\n",
        "        self.GitUsers.append(self.GUSER)\n",
        "        !curl https://api.github.com/users/{self.GUSER}/repos?per_page=100 | grep -o 'git@[^\"]*' > {root}/info.txt\n",
        "        cmdline('cat '+root+\"/info.txt |awk -F ':' '{print $2}'|awk -F '.' '{print $1}' > \"+self.path+\"/\"+self.GUSER+\"_repositories.txt\",True)\n",
        "        with open(root+'/info.txt','r') as f:\n",
        "            for line in f:\n",
        "              cline=line.split(':')[1].split('.')[0]\n",
        "              self.sub_repo_list.append(cline),\n",
        "\n",
        "      print(self.sub_repo_list)          \n",
        "\n",
        " \n",
        "\n",
        "      def __repr__(self):\n",
        "        return self.PATH\n",
        "\n",
        "G=GitGo(reps,sub_repos=True,path=root)\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "pGxVYslkcfKW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install reqs\n",
        "loot = recursive_glob(root,'req*.txt')\n",
        "print(loot)\n",
        "for l in loot:\n",
        "    !pip install -r {l}\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KL3ac85FiWOG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AQXt2kj-n1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5c786a-4749-4aa5-c5c1-dbf7a8f6f3b3"
      },
      "source": [
        "#@title Define paths\n",
        "#@markdown > choose local or gdrive als root filesystem\n",
        "\n",
        "\n",
        "# define folders\n",
        "input_folder=root+'/images_raw'\n",
        "output_folder=root+'/images_output'\n",
        "input_folder_resized=root+'/images_resized'\n",
        "# make folders\n",
        "os.makedirs(input_folder, exist_ok = True)\n",
        "os.makedirs(input_folder_resized, exist_ok = True)\n",
        "# count files\n",
        "listing_raw=os.listdir(input_folder)\n",
        "listing_resized=os.listdir(input_folder_resized)\n",
        "\n",
        "print('All repos have are installed on the '+filesystem+' filesystem!')\n",
        "\n",
        "# clear_output()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All repos have are installed on the local filesystem!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess images to set"
      ],
      "metadata": {
        "id": "yDKntZpDhw1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(root + '/CycleGAN')"
      ],
      "metadata": {
        "id": "AIjV0Iw6jY3d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kok_cA3aje0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(root + '/Keras-GAN')\n"
      ],
      "metadata": {
        "id": "SP1TDQpmSrdi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJOXQDiTWMEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sLvmcwUoDQW"
      },
      "source": [
        "keyword='face'\n",
        "\n",
        "#!gallery-dl -d {input_folder} https://flickr.com/search/?text={keyword}  --verbose -R 200 --sleep 0.04\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Q7I3Ee41ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9eae70a-065b-414c-cca8-cf18f4bb07d5"
      },
      "source": [
        "heap=input_folder+'/flickr/search/'+keyword\n",
        "print(heap)\n",
        "#print(input_folder)\n",
        "for file in glob.glob(heap+'/*'):\n",
        "    shutil.move(file, input_folder)\n",
        "    plt.show(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image_learning/images_raw/flickr/search/face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/image_learning/A1_CycleGAN-and-pix2pix_with_colab')"
      ],
      "metadata": {
        "id": "ryywek99Zmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data import image_folder as imfo\n",
        "imfo.make_dataset(heap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "bez0mpSkZajZ",
        "outputId": "56477f2b-8170-41f7-81e9-5b42b4115c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-28e2d38d9f15>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_folder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/image_learning/A1_CycleGAN-and-pix2pix_with_colab/data/image_folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(dir, max_dataset_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dataset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;31m#assert os.path.exists(dir), '%s is not a valid directory' % dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: /content/image_learning/images_raw/flickr/search/face is not a valid directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/image_learning/images_raw/flickr/Search/face',\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(256, 256),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation=\"bilinear\",\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_XmqFG2wgn6m",
        "outputId": "5744b839-d569-4115-c77b-5763027bdae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 files belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-204135bff594>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'/content/image_learning/images_raw/flickr/Search/face'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m         )\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;34mf\"No images found in directory {directory}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;34mf\"Allowed formats: {ALLOWLIST_FORMATS}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No images found in directory /content/image_learning/images_raw/flickr/Search/face. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "samples = 10\n",
        "for idx, c in enumerate(['/content/image_learning/images_raw/flickr/Search/face/', '/content/image_learning/images_raw/flickr/Search/rainforest/']*samples):\n",
        "  imarray = numpy.random.rand(100,100,3) * 255\n",
        "  im = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
        "  im.save('{}result_image{}.png'.format(c, idx))\n",
        "\n",
        "train_labels = [0]*samples + [1]*samples\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  '/content/data',\n",
        "  label_mode='int',\n",
        "  labels = train_labels,\n",
        "  shuffle=False,\n",
        "  seed=123,\n",
        "  image_size=(100, 100),\n",
        "  batch_size=4)\n",
        "\n",
        "for x, y in train_ds.take(1):\n",
        "  print(x.shape, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "N0SUSxPyehq3",
        "outputId": "60973330-7aa4-4c97-c858-7634a1842b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-bc6866786e5d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;34m'/content/data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0;34m\"Expected the lengths of `labels` to match the number \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;34m\"of files in the target directory. len(labels) is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 20 while we found 0 files in directory /content/data."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Ao8blKBQqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f73f3a-51b4-4900-bcf6-3b3f5b5a3673"
      },
      "source": [
        "!cat /content/gallery-dl/docs/gallery-dl.conf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/gallery-dl/docs/gallery-dl.conf: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi5fjxle9LNL"
      },
      "source": [
        "# !gallery-dl --list-extractors |grep 'http' > /content/gallery-dl/extractor_urls.txt\n",
        "# !gallery-dl --help\n",
        "\n",
        "# !youtube-dl --list-extractors |grep 'http' > /content/gallery-dl/extractor_urls.txt\n",
        "# !youtube-dl --help\n",
        "\n",
        "sage: gallery-dl [OPTION]... URL...\n",
        "\n",
        "General Options:\n",
        "  -h, --help                Print this help message and exit\n",
        "  --version                 Print program version and exit\n",
        "  -d, --dest DEST           Destination directory\n",
        "  -i, --input-file FILE     Download URLs found in FILE ('-' for stdin)\n",
        "  --cookies FILE            File to load additional cookies from\n",
        "  --proxy URL               Use the specified proxy\n",
        "  --clear-cache             Delete all cached login sessions, cookies, etc.\n",
        "\n",
        "Output Options:\n",
        "  -q, --quiet               Activate quiet mode\n",
        "  -v, --verbose             Print various debugging information\n",
        "  -g, --get-urls            Print URLs instead of downloading\n",
        "  -j, --dump-json           Print JSON information\n",
        "  -s, --simulate            Simulate data extraction; do not download anything\n",
        "  -K, --list-keywords       Print a list of available keywords and example\n",
        "                            values for the given URLs\n",
        "  --list-modules            Print a list of available extractor modules\n",
        "  --list-extractors         Print a list of extractor classes with\n",
        "                            description, (sub)category and example URL\n",
        "  --write-log FILE          Write logging output to FILE\n",
        "  --write-unsupported FILE  Write URLs, which get emitted by other extractors\n",
        "                            but cannot be handled, to FILE\n",
        "\n",
        "Downloader Options:\n",
        "  -r, --limit-rate RATE     Maximum download rate (e.g. 500k or 2.5M)\n",
        "  -R, --retries N           Maximum number of retries for failed HTTP requests\n",
        "                            or -1 for infinite retries (default: 4)\n",
        "  -A, --abort N             Abort extractor run after N consecutive file\n",
        "                            downloads have been skipped, e.g. if files with\n",
        "                            the same filename already exist\n",
        "  --http-timeout SECONDS    Timeout for HTTP connections (defaut: 30.0)\n",
        "  --sleep SECONDS           Number of seconds to sleep before each download\n",
        "  --no-part                 Do not use .part files\n",
        "  --no-mtime                Do not set file modification times according to\n",
        "                            Last-Modified HTTP response headers\n",
        "  --no-download             Do not download any files\n",
        "  --no-check-certificate    Disable HTTPS certificate validation\n",
        "\n",
        "Configuration Options:\n",
        "  -c, --config FILE         Additional configuration files\n",
        "  -o, --option OPT          Additional '<key>=<value>' option values\n",
        "  --ignore-config           Do not read the default configuration files\n",
        "\n",
        "Authentication Options:\n",
        "  -u, --username USER       Username to login with\n",
        "  -p, --password PASS       Password belonging to the given username\n",
        "  --netrc                   Enable .netrc authentication data\n",
        "\n",
        "Selection Options:\n",
        "  --download-archive FILE   Record all downloaded files in the archive file\n",
        "                            and skip downloading any file already in it.\n",
        "  --range RANGE             Index-range(s) specifying which images to\n",
        "                            download. For example '5-10' or '1,3-5,10-'\n",
        "  --chapter-range RANGE     Like '--range', but applies to manga-chapters and\n",
        "                            other delegated URLs\n",
        "  --filter EXPR             Python expression controlling which images to\n",
        "                            download. Files for which the expression evaluates\n",
        "                            to False are ignored. Available keys are the\n",
        "                            filename-specific ones listed by '-K'. Example:\n",
        "                            --filter \"image_width >= 1000 and rating in ('s',\n",
        "                            'q')\"\n",
        "  --chapter-filter EXPR     Like '--filter', but applies to manga-chapters and\n",
        "                            other delegated URLs\n",
        "\n",
        "Post-processing Options:\n",
        "  --zip                     Store downloaded files in a ZIP archive\n",
        "  --ugoira-conv             Convert Pixiv Ugoira to WebM (requires FFmpeg)\n",
        "  --write-metadata          Write metadata to separate JSON files\n",
        "  --write-tags              Write image tags to separate text files\n",
        "  --mtime-from-date         Set file modification times according to 'date'\n",
        "                            metadata"
      ]
    }
  ]
}