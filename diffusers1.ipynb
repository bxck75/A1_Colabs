{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxck75/A1_Colabs/blob/master/diffusers1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gkPaNEEczbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "dmHm7OgEcyZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio -U\n"
      ],
      "metadata": {
        "id": "cjo4fcnZKmvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hidiffusion\n",
        "!pip install -q diffusers==0.25.0 transformers==4.27.4 accelerate==0.18.0 xformers==0.0.16rc425"
      ],
      "metadata": {
        "id": "A_XHYDfaJxB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from hidiffusion import apply_hidiffusion, remove_hidiffusion\n",
        "from diffusers import StableDiffusionXLPipeline, DDIMScheduler\n",
        "import torch\n",
        "pretrain_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "scheduler = DDIMScheduler.from_pretrained(pretrain_model, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(pretrain_model, scheduler = scheduler, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "pipe.enable_vae_tiling()\n",
        "# Apply hidiffusion with a single line of code.\n",
        "apply_hidiffusion(pipe)\n",
        "prompt = \"thick strokes, bright colors, an exotic fox, cute, chibi kawaii. detailed fur, hyperdetailed , big reflective eyes, fairytale, artstation,centered composition, perfect composition, centered, vibrant colors, muted colors, high detailed, 8k.\"\n",
        "negative_prompt = \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\"\n",
        "image = pipe(prompt, guidance_scale=7.5, height=2048, width=2048, eta=1.0, negative_prompt=negative_prompt).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "qiKKKgJEJjFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C1RbMM2xJ4IT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxtlvnKkcsQL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "from diffusers.utils import load_image, export_to_video\n",
        "\n",
        "pipeline = StableVideoDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n",
        ")\n",
        "\n",
        "pipeline.enable_model_cpu_offload()\n",
        "\n",
        "\n",
        "\n",
        "image = load_image(\"https://radames-enhance-this-hidiffusion-sdxl.hf.space/file=/tmp/gradio/9c23bab0cd596a34f47a1867f205f82a634b5a2b287f4be8c6f587f495a71ce5/output_433.png\")\n",
        "image = image.resize((512, 512))\n",
        "\n",
        "generator = torch.manual_seed(42)\n",
        "frames = pipeline(image, decode_chunk_size=2, generator=generator).frames[0]\n",
        "export_to_video(frames, \"generated.mp4\", fps=10)"
      ]
    }
  ]
}