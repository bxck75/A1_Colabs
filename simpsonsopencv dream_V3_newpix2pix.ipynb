{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxck75/A1_Colabs/blob/master/simpsonsopencv%20dream_V3_newpix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/*"
      ],
      "metadata": {
        "id": "L17ZT9S6ZSid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os \n",
        "\n",
        "!curl https://raw.githubusercontent.com/bxck75/colab_starter/main/setup.py -o /content/setup.py\n",
        "\n",
        "!pip install tensorflow tensorflow-addons\n",
        "\n",
        "######################################################"
      ],
      "metadata": {
        "id": "fowXHME-f_Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/setup.py"
      ],
      "metadata": {
        "id": "r8Mw77AlHLm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/GitGo/datasets')\n",
        "!unzip /content/drive/MyDrive/garbage-20230421T030613Z-001.zip"
      ],
      "metadata": {
        "id": "GrPnBikSxpNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob,cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "!mkdir -p /content/GitGo/datasets/garbage_imgs_L\n",
        "!mkdir -p /content/GitGo/datasets/garbage_imgs_P\n",
        "\n",
        "i=0\n",
        "for _f in sorted(glob.glob('/content/GitGo/datasets/garbage/*/*.jpg')):\n",
        "  im = cv2.imread(_f)\n",
        "  \n",
        "  # You may need to convert the color.\n",
        "  color_converted = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "  pil_image=Image.fromarray(color_converted)\n",
        "  \n",
        "  h, w, c = im.shape\n",
        "  split_fn = _f.split('/')\n",
        "  print(split_fn[-1])\n",
        "  if w < h:\n",
        "    # portrait folder and rename to png\n",
        "    pil_image.save('/content/GitGo/datasets/garbage_imgs_P/'+ split_fn[-1].replace('.jpg','.png'))\n",
        "    i+=1\n",
        "  else:\n",
        "    # landscape folder and rename to png\n",
        "    pil_image.save('/content/GitGo/datasets/garbage_imgs_L/'+ split_fn[-1].replace('.jpg','.png'))\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "iHsMBHK6s6Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /tmp/preprocess/*\n",
        "!rm -r /content/GitGo/datasets/garbage_imgs_P/*\n",
        "!rm -r /content/GitGo/datasets/garbage_imgs_L/*"
      ],
      "metadata": {
        "id": "buIl-3LGBS7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/GitGo/datasets/garbage_imgs_P/*intro.png\n",
        "!rm -r /content/GitGo/datasets/garbage_imgs_P/*_ch*.png"
      ],
      "metadata": {
        "id": "uwFOUeH6uVMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Function to rename multiple files\n",
        "def rename():\n",
        "  i = 0\n",
        "  path=\"/content/GitGo/datasets/garbage_imgs/\"\n",
        "  for _filename in sorted(os.listdir(path)):\n",
        "    print(_filename)\n",
        "    my_source = path + _filename\n",
        "    my_dest = my_source.replace('.jpg','.png')\n",
        "    # rename() function will\n",
        "    # rename all the files\n",
        "    os.rename(my_source, my_dest)\n",
        "    i += 1\n"
      ],
      "metadata": {
        "id": "mmXT5WNkvqv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /tmp/preprocess/*"
      ],
      "metadata": {
        "id": "5fu_jll_epsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/GitGo/piss-ant-pix2pix')\n",
        "input_images = '/content/GitGo/datasets/garbage_imgs_P'"
      ],
      "metadata": {
        "id": "-C92QZ4y9oaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# resize\n",
        "!python tools/process_custom.py --size 400 --pad --input_dir {input_images} --operation resize --output_dir /tmp/preprocess/_resized\n",
        "# blank clones\n",
        "#!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --operation blank --output_dir /tmp/preprocess/_blank\n",
        "# edge clones\n",
        "!mkdir -p /tmp/preprocess/_edges\n",
        "!python /content/GitGo/piss-ant-pix2pix/tools/edge_custom.py --input_dir /tmp/preprocess/_resized --output_dir /tmp/preprocess/_edges\n",
        "#!python tools/process_custom.py --input_dir /tmp/preprocess/_edge --operation blank --output_dir /tmp/preprocess/_blank_edge\n",
        "# combine\n",
        "#!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_blank --operation combine --output_dir  /tmp/preprocess/_resized/_combined_blank\n",
        "!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_edges --operation combine --output_dir  /tmp/preprocess/_resized/_combined_edge\n",
        "#!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_blank_edge --operation combine --output_dir  /tmp/preprocess/_resized/_combined_blank_edge\n",
        "# split\n",
        "#!python tools/split.py --dir /tmp/preprocess/_resized/_combined_blank\n",
        "!python tools/split.py --dir /tmp/preprocess/_resized/_combined_edge\n",
        "#!python tools/split.py --dir /tmp/preprocess/_resized/_combined_blank_edge\n",
        "# make outdirs\n",
        "#!mkdir -p /content/GitGo/datasets/garbage_blank_dataset\n",
        "!mkdir -p /content/GitGo/datasets/simpsons_edge_dataset\n",
        "#!mkdir -p /content/GitGo/datasets/simpsons_blank_edge_dataset\n",
        "# copy to final folders\n",
        "#!cp -rf /tmp/preprocess/_resized/_combined_blank/ /content/GitGo/datasets/garbage_blank_dataset\n",
        "!cp -rf /tmp/preprocess/_resized/_combined_edge/ /content/GitGo/datasets/garbage_edge_dataset\n",
        "#!cp -rf /tmp/preprocess/_resized/_combined_blank_edge/ /content/GitGo/datasets/simpsons_blank_edge_dataset"
      ],
      "metadata": {
        "id": "FRuNcHxqE9vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DeepDream"
      ],
      "metadata": {
        "id": "JORfNDIiwLXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "if \"/content/GitGo/TensorFlow-Tutorials\" not in sys.path:\n",
        "  sys.path.insert(1, \"/content/GitGo/TensorFlow-Tutorials\")\n",
        "  print(sys.path)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import PIL.Image\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import inception5h\n",
        "import importlib\n",
        "\n",
        "importlib.reload(inception5h)\n",
        "\n",
        "\n",
        "!mkdir -p /tmp/preprocess/_dream\n",
        "\n",
        "inception5h.maybe_download()\n",
        "model = inception5h.Inception5h()\n",
        "\n",
        "#len(model.layer_tensors)\n",
        "#print(model.layer_tensors)\n",
        "#tensors = ['conv2d0:0', 'conv2d1:0', 'conv2d2:0', 'mixed3a:0', 'mixed3b:0', 'mixed4a:0', 'mixed4b:0', 'mixed4c:0', 'mixed4d:0', 'mixed4e:0', 'mixed5a:0', 'mixed5b:0']"
      ],
      "metadata": {
        "id": "8FtoBQdOFvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DreamMethods"
      ],
      "metadata": {
        "id": "FapBsAUbMdD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(filename):\n",
        "    image = PIL.Image.open(filename)\n",
        "\n",
        "    return np.float32(image)\n",
        "    \n",
        "def save_image(image, filename):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "    \n",
        "    # Convert to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    # Write the image-file in jpeg-format.\n",
        "    with open(filename, 'wb') as file:\n",
        "        PIL.Image.fromarray(image).save(file, 'jpeg')"
      ],
      "metadata": {
        "id": "Zn0RC8HBKq_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image):\n",
        "    # Assume the pixel-values are scaled between 0 and 255.\n",
        "    \n",
        "    if False:\n",
        "        # Convert the pixel-values to the range between 0.0 and 1.0\n",
        "        image = np.clip(image/255.0, 0.0, 1.0)\n",
        "        \n",
        "        # Plot using matplotlib.\n",
        "        plt.imshow(image, interpolation='lanczos')\n",
        "        plt.show()\n",
        "    else:\n",
        "        # Ensure the pixel-values are between 0 and 255.\n",
        "        image = np.clip(image, 0.0, 255.0)\n",
        "        \n",
        "        # Convert pixels to bytes.\n",
        "        image = image.astype(np.uint8)\n",
        "\n",
        "        # Convert to a PIL-image and display it.\n",
        "        display(PIL.Image.fromarray(image))\n",
        "#Normalize an image so its values are between 0.0 and 1.0. This is useful for plotting the gradient.\n",
        "\n",
        "def normalize_image(x):\n",
        "    # Get the min and max values for all pixels in the input.\n",
        "    x_min = x.min()\n",
        "    x_max = x.max()\n",
        "\n",
        "    # Normalize so all values are between 0.0 and 1.0\n",
        "    x_norm = (x - x_min) / (x_max - x_min)\n",
        "    \n",
        "    return x_norm\n",
        "#This function plots the gradient after normalizing it.\n",
        "\n",
        "def plot_gradient(gradient):\n",
        "    # Normalize the gradient so it is between 0.0 and 1.0\n",
        "    gradient_normalized = normalize_image(gradient)\n",
        "    \n",
        "    # Plot the normalized gradient.\n",
        "    plt.imshow(gradient_normalized, interpolation='bilinear')\n",
        "    plt.show()\n",
        "#This function resizes an image. It can take a size-argument where you give it the exact pixel-size you want the image to be e.g. (100, 200). Or it can take a factor-argument where you give it the rescaling-factor you want to use e.g. 0.5 for halving the size of the image in each dimension.\n",
        "\n",
        "#This is implemented using PIL which is a bit lengthy because we are working on numpy arrays where the pixels are floating-point values. This is not supported by PIL so the image must be converted to 8-bit bytes while ensuring the pixel-values are within the proper limits. Then the image is resized and converted back to floating-point values.\n",
        "\n",
        "def resize_image(image, size=None, factor=None):\n",
        "    # If a rescaling-factor is provided then use it.\n",
        "    if factor is not None:\n",
        "        # Scale the numpy array's shape for height and width.\n",
        "        size = np.array(image.shape[0:2]) * factor\n",
        "        \n",
        "        # The size is floating-point because it was scaled.\n",
        "        # PIL requires the size to be integers.\n",
        "        size = size.astype(int)\n",
        "    else:\n",
        "        # Ensure the size has length 2.\n",
        "        size = size[0:2]\n",
        "    \n",
        "    # The height and width is reversed in numpy vs. PIL.\n",
        "    size = tuple(reversed(size))\n",
        "\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    img = np.clip(image, 0.0, 255.0)\n",
        "    \n",
        "    # Convert the pixels to 8-bit bytes.\n",
        "    img = img.astype(np.uint8)\n",
        "    \n",
        "    # Create PIL-object from numpy array.\n",
        "    img = PIL.Image.fromarray(img)\n",
        "    \n",
        "    # Resize the image.\n",
        "    img_resized = img.resize(size, PIL.Image.LANCZOS)\n",
        "    \n",
        "    # Convert 8-bit pixel values back to floating-point.\n",
        "    img_resized = np.float32(img_resized)\n",
        "\n",
        "    return img_resized"
      ],
      "metadata": {
        "id": "wChdbnsSL_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tile_size(num_pixels, tile_size=400):\n",
        "    \"\"\"\n",
        "    num_pixels is the number of pixels in a dimension of the image.\n",
        "    tile_size is the desired tile-size.\n",
        "    \"\"\"\n",
        "\n",
        "    # How many times can we repeat a tile of the desired size.\n",
        "    num_tiles = int(round(num_pixels / tile_size))\n",
        "    \n",
        "    # Ensure that there is at least 1 tile.\n",
        "    num_tiles = max(1, num_tiles)\n",
        "    \n",
        "    # The actual tile-size.\n",
        "    actual_tile_size = math.ceil(num_pixels / num_tiles)\n",
        "    \n",
        "    return actual_tile_size\n",
        "#This helper-function computes the gradient for an input image. The image is split into tiles and the gradient is calculated for each tile. The tiles are chosen randomly to avoid visible seams / lines in the final DeepDream image.\n",
        "\n",
        "def tiled_gradient(gradient, image, tile_size=400):\n",
        "    # Allocate an array for the gradient of the entire image.\n",
        "    grad = np.zeros_like(image)\n",
        "\n",
        "    # Number of pixels for the x- and y-axes.\n",
        "    x_max, y_max, _ = image.shape\n",
        "\n",
        "    # Tile-size for the x-axis.\n",
        "    x_tile_size = get_tile_size(num_pixels=x_max, tile_size=tile_size)\n",
        "    # 1/4 of the tile-size.\n",
        "    x_tile_size4 = x_tile_size // 4\n",
        "\n",
        "    # Tile-size for the y-axis.\n",
        "    y_tile_size = get_tile_size(num_pixels=y_max, tile_size=tile_size)\n",
        "    # 1/4 of the tile-size\n",
        "    y_tile_size4 = y_tile_size // 4\n",
        "\n",
        "    # Random start-position for the tiles on the x-axis.\n",
        "    # The random value is between -3/4 and -1/4 of the tile-size.\n",
        "    # This is so the border-tiles are at least 1/4 of the tile-size,\n",
        "    # otherwise the tiles may be too small which creates noisy gradients.\n",
        "    x_start = random.randint(-3*x_tile_size4, -x_tile_size4)\n",
        "\n",
        "    while x_start < x_max:\n",
        "        # End-position for the current tile.\n",
        "        x_end = x_start + x_tile_size\n",
        "        \n",
        "        # Ensure the tile's start- and end-positions are valid.\n",
        "        x_start_lim = max(x_start, 0)\n",
        "        x_end_lim = min(x_end, x_max)\n",
        "\n",
        "        # Random start-position for the tiles on the y-axis.\n",
        "        # The random value is between -3/4 and -1/4 of the tile-size.\n",
        "        y_start = random.randint(-3*y_tile_size4, -y_tile_size4)\n",
        "\n",
        "        while y_start < y_max:\n",
        "            # End-position for the current tile.\n",
        "            y_end = y_start + y_tile_size\n",
        "\n",
        "            # Ensure the tile's start- and end-positions are valid.\n",
        "            y_start_lim = max(y_start, 0)\n",
        "            y_end_lim = min(y_end, y_max)\n",
        "\n",
        "            # Get the image-tile.\n",
        "            img_tile = image[x_start_lim:x_end_lim,\n",
        "                             y_start_lim:y_end_lim, :]\n",
        "\n",
        "            # Create a feed-dict with the image-tile.\n",
        "            feed_dict = model.create_feed_dict(image=img_tile)\n",
        "\n",
        "            # Use TensorFlow to calculate the gradient-value.\n",
        "            g = session.run(gradient, feed_dict=feed_dict)\n",
        "\n",
        "            # Normalize the gradient for the tile. This is\n",
        "            # necessary because the tiles may have very different\n",
        "            # values. Normalizing gives a more coherent gradient.\n",
        "            g /= (np.std(g) + 1e-8)\n",
        "\n",
        "            # Store the tile's gradient at the appropriate location.\n",
        "            grad[x_start_lim:x_end_lim,\n",
        "                 y_start_lim:y_end_lim, :] = g\n",
        "            \n",
        "            # Advance the start-position for the y-axis.\n",
        "            y_start = y_end\n",
        "\n",
        "        # Advance the start-position for the x-axis.\n",
        "        x_start = x_end\n",
        "\n",
        "    return grad\n",
        "#Optimize Image\n",
        "#This function is the main optimization-loop for the DeepDream algorithm. It calculates the gradient of the given layer of the Inception model with regard to the input image. The gradient is then added to the input image so the mean value of the layer-tensor is increased. This process is repeated a number of times and amplifies whatever patterns the Inception model sees in the input image.\n",
        "\n",
        "def optimize_image(layer_tensor, image,\n",
        "                   num_iterations=10, step_size=3.0, tile_size=400,\n",
        "                   show_gradient=False):\n",
        "    \"\"\"\n",
        "    Use gradient ascent to optimize an image so it maximizes the\n",
        "    mean value of the given layer_tensor.\n",
        "    \n",
        "    Parameters:\n",
        "    layer_tensor: Reference to a tensor that will be maximized.\n",
        "    image: Input image used as the starting point.\n",
        "    num_iterations: Number of optimization iterations to perform.\n",
        "    step_size: Scale for each step of the gradient ascent.\n",
        "    tile_size: Size of the tiles when calculating the gradient.\n",
        "    show_gradient: Plot the gradient in each iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copy the image so we don't overwrite the original image.\n",
        "    img = image.copy()\n",
        "    \n",
        "    print(\"Image before:\")\n",
        "    plot_image(img)\n",
        "\n",
        "    print(\"Processing image: \", end=\"\")\n",
        "\n",
        "    # Use TensorFlow to get the mathematical function for the\n",
        "    # gradient of the given layer-tensor with regard to the\n",
        "    # input image. This may cause TensorFlow to add the same\n",
        "    # math-expressions to the graph each time this function is called.\n",
        "    # It may use a lot of RAM and could be moved outside the function.\n",
        "    gradient = model.get_gradient(layer_tensor)\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # Calculate the value of the gradient.\n",
        "        # This tells us how to change the image so as to\n",
        "        # maximize the mean of the given layer-tensor.\n",
        "        grad = tiled_gradient(gradient=gradient, image=img,\n",
        "                              tile_size=tile_size)\n",
        "        \n",
        "        # Blur the gradient with different amounts and add\n",
        "        # them together. The blur amount is also increased\n",
        "        # during the optimization. This was found to give\n",
        "        # nice, smooth images. You can try and change the formulas.\n",
        "        # The blur-amount is called sigma (0=no blur, 1=low blur, etc.)\n",
        "        # We could call gaussian_filter(grad, sigma=(sigma, sigma, 0.0))\n",
        "        # which would not blur the colour-channel. This tends to\n",
        "        # give psychadelic / pastel colours in the resulting images.\n",
        "        # When the colour-channel is also blurred the colours of the\n",
        "        # input image are mostly retained in the output image.\n",
        "        sigma = (i * 4.0) / num_iterations + 0.5\n",
        "        grad_smooth1 = gaussian_filter(grad, sigma=sigma)\n",
        "        grad_smooth2 = gaussian_filter(grad, sigma=sigma*2)\n",
        "        grad_smooth3 = gaussian_filter(grad, sigma=sigma*0.5)\n",
        "        grad = (grad_smooth1 + grad_smooth2 + grad_smooth3)\n",
        "\n",
        "        # Scale the step-size according to the gradient-values.\n",
        "        # This may not be necessary because the tiled-gradient\n",
        "        # is already normalized.\n",
        "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
        "\n",
        "        # Update the image by following the gradient.\n",
        "        img += grad * step_size_scaled\n",
        "\n",
        "        if show_gradient:\n",
        "            # Print statistics for the gradient.\n",
        "            msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
        "            print(msg.format(grad.min(), grad.max(), step_size_scaled))\n",
        "\n",
        "            # Plot the gradient.\n",
        "            plot_gradient(grad)\n",
        "        else:\n",
        "            # Otherwise show a little progress-indicator.\n",
        "            print(\". \", end=\"\")\n",
        "\n",
        "    print()\n",
        "    print(\"Image after:\")\n",
        "    plot_image(img)\n",
        "    \n",
        "    return img\n",
        "#Recursive Image Optimization\n",
        "#The Inception model was trained on fairly small images. The exact size is unclear but maybe 200-300 pixels in each dimension. If we use larger images such as 1920x1080 pixels then the optimize_image() function above will add many small patterns to the image.\n",
        "\n",
        "#This helper-function downscales the input image several times and runs each downscaled version through the optimize_image() function above. This results in larger patterns in the final image. It also speeds up the computation.\n",
        "\n",
        "def recursive_optimize(layer_tensor, image,\n",
        "                       num_repeats=4, rescale_factor=0.7, blend=0.2,\n",
        "                       num_iterations=10, step_size=3.0,\n",
        "                       tile_size=400):\n",
        "    \"\"\"\n",
        "    Recursively blur and downscale the input image.\n",
        "    Each downscaled image is run through the optimize_image()\n",
        "    function to amplify the patterns that the Inception model sees.\n",
        "\n",
        "    Parameters:\n",
        "    image: Input image used as the starting point.\n",
        "    rescale_factor: Downscaling factor for the image.\n",
        "    num_repeats: Number of times to downscale the image.\n",
        "    blend: Factor for blending the original and processed images.\n",
        "\n",
        "    Parameters passed to optimize_image():\n",
        "    layer_tensor: Reference to a tensor that will be maximized.\n",
        "    num_iterations: Number of optimization iterations to perform.\n",
        "    step_size: Scale for each step of the gradient ascent.\n",
        "    tile_size: Size of the tiles when calculating the gradient.\n",
        "    \"\"\"\n",
        "\n",
        "    # Do a recursive step?\n",
        "    if num_repeats>0:\n",
        "        # Blur the input image to prevent artifacts when downscaling.\n",
        "        # The blur amount is controlled by sigma. Note that the\n",
        "        # colour-channel is not blurred as it would make the image gray.\n",
        "        sigma = 0.5\n",
        "        img_blur = gaussian_filter(image, sigma=(sigma, sigma, 0.0))\n",
        "\n",
        "        # Downscale the image.\n",
        "        img_downscaled = resize_image(image=img_blur,\n",
        "                                      factor=rescale_factor)\n",
        "            \n",
        "        # Recursive call to this function.\n",
        "        # Subtract one from num_repeats and use the downscaled image.\n",
        "        img_result = recursive_optimize(layer_tensor=layer_tensor,\n",
        "                                        image=img_downscaled,\n",
        "                                        num_repeats=num_repeats-1,\n",
        "                                        rescale_factor=rescale_factor,\n",
        "                                        blend=blend,\n",
        "                                        num_iterations=num_iterations,\n",
        "                                        step_size=step_size,\n",
        "                                        tile_size=tile_size)\n",
        "        \n",
        "        # Upscale the resulting image back to its original size.\n",
        "        img_upscaled = resize_image(image=img_result, size=image.shape)\n",
        "\n",
        "        # Blend the original and processed images.\n",
        "        image = blend * image + (1.0 - blend) * img_upscaled\n",
        "\n",
        "    print(\"Recursive level:\", num_repeats)\n",
        "\n",
        "    # Process the image using the DeepDream algorithm.\n",
        "    img_result = optimize_image(layer_tensor=layer_tensor,\n",
        "                                image=image,\n",
        "                                num_iterations=num_iterations,\n",
        "                                step_size=step_size,\n",
        "                                tile_size=tile_size)\n",
        "    \n",
        "    return img_result"
      ],
      "metadata": {
        "id": "AUGgmfClMQaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dream images"
      ],
      "metadata": {
        "id": "RH2f9W0Hm51d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#session = tf.InteractiveSession(graph=model.graph)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "session = tf.compat.v1.Session(graph=model.graph)\n",
        "def Go_Dream(folder, _tensor=3):\n",
        "  \n",
        "  for df in sorted(glob.glob(folder+'/*.png')):\n",
        "    print(df)\n",
        "    new_name = df.replace('_resized','_dream')\n",
        "    print(new_name)\n",
        "    #file_name = '/content/GitGo/datasets/garbage/llery_2k13mini/13mini_196b.jpg'\n",
        "    image = load_image(filename=df)\n",
        "    #plot_image(image)\n",
        "    layer_tensor = model.layer_tensors[random.randrange(0, len(model.layer_tensors) - 1 )][:,:,:,0:3]\n",
        "    layer_tensor\n",
        "    img_result = recursive_optimize(layer_tensor=layer_tensor, image=image,\n",
        "                    num_iterations=5, step_size=3.5, rescale_factor=0.902,\n",
        "                    num_repeats=2, blend=0.005)\n",
        "    \n",
        "    layer_tensor = model.layer_tensors[random.randrange(0, len(model.layer_tensors) - 1 )]\n",
        "    \n",
        "    img_result_deep = optimize_image(layer_tensor, img_result,\n",
        "                   num_iterations=5, step_size=2.0, tile_size=200,\n",
        "                   show_gradient=False)\n",
        "    nsplit = df.split('/')\n",
        "   # new_name = df.replace('_resized','_dream')].replace('.png','.jpg')\n",
        "    new_name ='/content/drive/MyDrive/DataSets/GPK/_dream/' + nsplit[-1].replace('.png','.jpg')\n",
        "    print(new_name)\n",
        "    save_image(img_result_deep, new_name)\n",
        "\n",
        "Go_Dream('/tmp/preprocess/_resized',4)"
      ],
      "metadata": {
        "id": "fLxZyn_LMvsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get faces only"
      ],
      "metadata": {
        "id": "vFG1ozW_m-0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/GitGo  && git clone https://github.com/kenxdrgn/Face-Detection-And-Auto-Crop"
      ],
      "metadata": {
        "id": "UsX_lXN2TL1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/GitGo/Face-Detection-And-Auto-Crop/requirements.txt"
      ],
      "metadata": {
        "id": "vOQj5C2_6SEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/GitGo/Face-Detection-And-Auto-Crop/')\n",
        "for g in sorted(glob.glob('/tmp/preprocess/_resized/*.png')):\n",
        "  !python /content/GitGo/Face-Detection-And-Auto-Crop/auto_crop.py {g}"
      ],
      "metadata": {
        "id": "sDZyvNgI8a8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "  \n",
        "# Read the input image\n",
        "img = cv2.imread('/content/GitGo/datasets/garbage_edge_dataset/val/13mini_197b.png')\n",
        "  \n",
        "# Convert into grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('/content/GitGo/opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
        "  \n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(img, (x, y), (x+w, y+h), \n",
        "                  (0, 0, 255), 2)\n",
        "      \n",
        "    face = img[y:y + h, x:x + w]\n",
        "    cv2_imshow(face)\n",
        "    cv2.imwrite('face.jpg', face)\n",
        "  \n",
        "cv2_imshow(img)\n",
        "cv2.waitKey()"
      ],
      "metadata": {
        "id": "ZmwjbomtnDrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/opencv/opencv.git\n",
        "!cd /content/GitGo/opencv"
      ],
      "metadata": {
        "id": "BFppsc-Z0cE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/GitGo/opencv/data/lbpcascades')\n",
        "!ls\n",
        "#!curl https://raw.githubusercontent.com/aalind0/OpenCV-Anime-Face-Detector/master/Anime%20Face%20Detector/lbpcascade_animeface.xml -o lbpcascade_animeface.xml"
      ],
      "metadata": {
        "id": "XnU4VOJk1e6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Haar Cascade Image Feature Detection\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# loading in the cascades.\n",
        "face_cascade = cv2.CascadeClassifier('/content/GitGo/opencv/data/lbpcascades/lbpcascade_animeface.xml')\n",
        "\n",
        "eye_cascade = cv2.CascadeClassifier('/content/GitGo/opencv/data/haarcascades/haarcascade_eye.xml')\n",
        "\n",
        "# Reading the image.\n",
        "img = cv2.imread('/content/drive/MyDrive/DataSets/GPK/_dream/13mini_197b.jpg')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# loading in the cascades.\n",
        "face_cascade = cv2.CascadeClassifier('/content/GitGo/opencv/data/lbpcascades/lbpcascade_animeface.xml')\n",
        "\n",
        "eye_cascade = cv2.CascadeClassifier('/content/GitGo/opencv/data/haarcascades/haarcascade_eye.xml')\n",
        "\n",
        "# Reading the image.\n",
        "img = cv2.imread('/content/GitGo/datasets/garbage_imgs_L/16promo_2.png')\n",
        "\n",
        "# detection and drawing rectangles\n",
        "while True:\n",
        "    #converting the image to grayscale for easier processing.\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_color = img[y:y+h, x:x+w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "        for (ex, ey, ew, eh) in eyes:\n",
        "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0,255,0), 2)\n",
        "      \n",
        "    # Press 'ESC' to release the camera.        \n",
        "    cv2_imshow(img)\n",
        "    k = cv2.waitKey(30) & 0xff\n",
        "    if k == 27:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "iNA2bheUqPXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/GitGo/piss-ant-pix2pix/tools/edge_custom.py --input_dir /tmp/preprocess/_resized --output_dir /tmp/preprocess/_edges\n",
        "\n",
        "!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_edges --operation combine --output_dir  /tmp/preprocess/_combined_edge\n",
        "!python tools/split.py --dir /tmp/preprocess/_combined_edge\n",
        "!mkdir -p /content/GitGo/datasets/garbage_edge_dataset\n",
        "!cp -rf /tmp/preprocess/_combined_edge/ /content/GitGo/datasets/garbage_edge_dataset"
      ],
      "metadata": {
        "id": "WgN7pPisuUEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GitGo/piss-ant-pix2pix\n",
        "import cv2,glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "!mkdir -p '/tmp/preprocess/_edges'\n",
        "\n",
        "\n",
        "#for _f in sorted(glob.glob('/content/GitGo/datasets/garbage/*/*.jpg')):\n",
        "def edge(_in,_out):\n",
        "    image = cv2.imread(_in)\n",
        "    #cv2_imshow(image)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    # show the original and blurred images\n",
        "    #cv2_imshow(image)\n",
        "    #cv2_imshow( blurred)\n",
        "    # compute a \"wide\", \"mid-range\", and \"tight\" threshold for the edges\n",
        "    # using the Canny edge detector\n",
        "    #wide = cv2.Canny(blurred, 10, 200)\n",
        "    mid = cv2.Canny(blurred, 30, 150)\n",
        "    #tight = cv2.Canny(blurred, 240, 250)\n",
        "    # show the output Canny edge maps\n",
        "    #cv2_imshow(~wide)\n",
        "    #cv2_imshow(~mid)\n",
        "    #cv2_imshow(~tight)\n",
        "    print(_in)\n",
        "    # You may need to convert the color.\n",
        "    color_converted = cv2.cvtColor(~mid, cv2.COLOR_BGR2RGB)\n",
        "    pil_image=Image.fromarray(color_converted)\n",
        "    pil_image.save(_out)\n",
        "\n",
        "    #cv2.imwrite(_out, ~mid)\n",
        "\n",
        "\n",
        "for img_path in sorted(glob.glob('/tmp/preprocess/_resized/*/*.jpg')):\n",
        "    print(img_path)\n",
        "    new_img_path = img_path.replace('_resized','_edges')\n",
        "    print(new_img_path)\n",
        "    edge(img_path, new_img_path)\n",
        "\n",
        "\n",
        "!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_edges --operation combine --output_dir  /tmp/preprocess/_combined_edge\n",
        "!python tools/split.py --dir /tmp/preprocess/_combined_edge\n",
        "!mkdir -p /content/GitGo/datasets/garbage_edge_dataset\n",
        "!cp -rf /tmp/preprocess/_combined_edge/ /content/GitGo/datasets/garbage_edge_dataset"
      ],
      "metadata": {
        "id": "RVMZCuQiojpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.path.exists('/tmp/preprocess/_edges'))\n",
        "!ls /tmp/preprocess/_edges"
      ],
      "metadata": {
        "id": "D1h7ASsbyohd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len( glob.glob('/tmp/preprocess/_edges/*.png')))\n",
        "edges_imgs = glob.glob('/tmp/preprocess/_edges/*.png')"
      ],
      "metadata": {
        "id": "mh2AY8wzndiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p  /tmp/preprocess/_blank_edge\n",
        "!python tools/process_custom.py --input_dir /tmp/preprocess/_edges --operation blank --output_dir /tmp/preprocess/_blank_edge\n",
        "!python tools/process_custom.py --input_dir /tmp/preprocess/_resized --b_dir  /tmp/preprocess/_blank_edge --operation combine --output_dir  /tmp/preprocess/_combined_blank_edge\n",
        "!python tools/split.py --dir /tmp/preprocess/_combined_blank_edge\n",
        "!mkdir -p /content/GitGo/datasets/garbage_blank_edge_dataset\n",
        "!cp -rf /tmp/preprocess/_combined_blank_edge /content/GitGo/datasets/garbage_blank_edge_dataset"
      ],
      "metadata": {
        "id": "zfyzAYe4vgy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "files = glob.glob('/content/GitGo/datasets/garbage_edge_dataset/_combined_edge/train/*.png')\n",
        "print(len(files))"
      ],
      "metadata": {
        "id": "lzEtfDFadYoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/drive/MyDrive/garbage_metrics/*"
      ],
      "metadata": {
        "id": "9JZgFzxm9CKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorboard-plugin-profile\n",
        "%reload_ext tensorboard\n",
        "logs_base_dir = '/content/garbage_metrics'\n",
        "os.makedirs(logs_base_dir, exist_ok=True)\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "metadata": {
        "id": "IC137HRr-_0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "warnings.filterwarnings('ignore')\n",
        "os.chdir('/content/GitGo/piss-ant-pix2pix')\n",
        "custom_dataset = '/content/GitGo/datasets/garbage_blank_dataset/_combined_blank/train'\n",
        "train_metrics = '/content/garbage_metrics'\n",
        "!mkdir -p {train_metrics}\n",
        "\n",
        "# CHECK IF ITS A RETRAINING\n",
        "if os.path.exists('/content/simpsons_metrics/checkpoint'):\n",
        "    retrain = '--checkpoint '+ train_metrics\n",
        "else:\n",
        "    retrain = ''\n",
        "# train\n",
        "!python pix2pix_custom.py \\\n",
        "    --summary_freq 50 \\\n",
        "    --ngf 64 \\\n",
        "    --ndf 64 \\\n",
        "    --save_freq 50 \\\n",
        "    --display_freq 10 \\\n",
        "    --progress_freq 50 \\\n",
        "    --batch_size 64 \\\n",
        "    --mode train \\\n",
        "    --output_dir {train_metrics} \\\n",
        "    --max_epochs 3 \\\n",
        "    --input_dir {custom_dataset} \\\n",
        "    {retrain} \\\n",
        "    --which_direction BtoA"
      ],
      "metadata": {
        "id": "C9ItyLe9Vk3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/garbage_metrics/images"
      ],
      "metadata": {
        "id": "MYfDwaF5qezf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/garbage_metrics_backup.zip /content/garbage_metrics"
      ],
      "metadata": {
        "id": "8fPIywSVgpkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}