{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxck75/A1_Colabs/blob/master/piss_ant_pix2pix_pailkids4000_latest_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRqbekgSPIVi"
      },
      "source": [
        "!rm -r /content/sample_data\n",
        "\n",
        "#imports\n",
        "import os,sys\n",
        "import fnmatch\n",
        "import shutil\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# list of reps to install\n",
        "reps=[\n",
        "#         'EIGENREPS',\n",
        "        'bxck75/piss-ant-pix2pix',\n",
        "#         'mikf/gallery-dl',\n",
        "#         'bxck75/datasets',\n",
        "#         'EIGENREPS',\n",
        "#         'tjwei/Flappy-Turtle.',\n",
        "#         'tjwei/fonttools',\n",
        "#         'tjwei/blender3d_import_psk_psa',\n",
        "#         'lllyasviel/sketchKeras',\n",
        "#         'Mckinsey666/Anime-Face-Dataset',\n",
        "#         'chenyuntc/pytorch-book',\n",
        "#         'lllyasviel/style2paints',\n",
        "#         'llSourcell/GANS-for-style-transfer',\n",
        "#         'opencv/open_model_zoo',\n",
        "#         'hindupuravinash/the-gan-zoo',\n",
        "#         'corenel/GAN-Zoo',\n",
        "#         'eriklindernoren/Keras-GAN',\n",
        "#         'junyanz/CycleGAN',\n",
        "#         'junyanz/pytorch-CycleGAN-and-pix2pix',\n",
        "#         'junyanz/iGAN', #----> !wget http://efrosgans.eecs.berkeley.edu/iGAN/datasets/church_64.zip <----dataset \toutdoor_128.zip \thandbag_128.zip !!!\n",
        "#         'martinarjovsky/WassersteinGAN',\n",
        "#         'shaoanlu/faceswap-GAN',\n",
        "#         'LantaoYu/SeqGAN',\n",
        "#         'tjwei/GANotebooks',\n",
        "#         'adeshpande3/Tensorflow-Programs-and-Tutorials',\n",
        "#         'adeshpande3/Generative-Adversarial-Networks',\n",
        "#         'diegoalejogm/gans',\n",
        "#         'osh/KerasGAN',\n",
        "#         'tensorflow/gan',\n",
        "#         'r9y9/gantts',\n",
        "#         'jayleicn/animeGAN',\n",
        "#         'jayleicn/ImageNet-Training',\n",
        "#         'Zardinality/WGAN-tensorflow',\n",
        "#         'timsainb/Tensorflow-MultiGPU-VAE-GAN',\n",
        "#         'Larox/python-moviepy-meetup',\n",
        "#         'tjwei/keras-yolo3',\n",
        "#         'tensorflow/moonlight'\n",
        "#         'tensorflow/models',\n",
        "#         'tensorflow/datasets',\n",
        "#        'tensorflow/docs',\n",
        "]\n",
        "print(reps.sort())\n",
        "\n",
        "# Gitgo class\n",
        "class GitGo():\n",
        "    \n",
        "  def __init__(self,rep,chdir=True,path='/content/'):\n",
        "    self.rep = rep\n",
        "    self.chdir = chdir\n",
        "    self.path = path\n",
        "    !mkdir -p {self.path}\n",
        "    if self.rep == 'help':\n",
        "        self.help()\n",
        "    self.go() \n",
        "      \n",
        "  def help(self):\n",
        "    print(\"* pulls git rep and shows files \\\n",
        "            * returns root path for the repository \\\n",
        "            * Function needs repository <user>/<repository name> combination\\\n",
        "            * Switch chdir and define the rootpath for the repository\\\n",
        "            * Use : GitGo('<rep owner>/<rep name>', <True/False>, <root path>)\\\n",
        "          \")\n",
        "    sys.exit()\n",
        "    \n",
        "  def go(self):\n",
        "    self.rep=self.rep.split('/')\n",
        "    # change folder check\n",
        "    if self.chdir ==True:\n",
        "        #Switch to path\n",
        "        os.chdir(self.path)\n",
        "    # pull get the git rep\n",
        "    os.system('git clone https://github.com/'+self.rep[0]+'/'+self.rep[1]+'.git')\n",
        "    # Set the return value for rep rootpath\n",
        "    self.PATH=self.path+self.rep[1]\n",
        "    # show imported files\n",
        "    os.system('ls ' + self.PATH)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.PATH\n",
        "\n",
        "\n",
        "def get_other_reps(reps):\n",
        "    GitUsers=[]\n",
        "    for u in reps:    \n",
        "        username=u.split('/')[0]\n",
        "        GitUsers.append(username)\n",
        "        GHUSER=username\n",
        "        !curl 'https://api.github.com/users/{GHUSER}/repos?per_page=100' | grep -o 'git@[^\"]*' \n",
        "\n",
        "    return list(set(GitUsers))\n",
        "\n",
        "def recursive_glob(treeroot, pattern):\n",
        "    results = []\n",
        "    for base, dirs, files in os.walk(treeroot):\n",
        "        goodfiles = fnmatch.filter(files, pattern)\n",
        "        results.extend(os.path.join(base, f) for f in goodfiles)\n",
        "    return results\n",
        "\n",
        "def MakeDirs(dirsList):\n",
        "    for d in dirsList:\n",
        "        if os.path.exists(d) == False:\n",
        "            os.makedirs(d)\n",
        "\n",
        "mylist = []\n",
        "# install all reps in the list\n",
        "for rep in reps:\n",
        "    name=rep.split('.')[0]\n",
        "    print(name)\n",
        "    G=GitGo(name,path='/content/installed_resp')\n",
        "    R=get_other_reps([name])\n",
        "#     print(R)\n",
        "    mylist.append([R[0],name])\n",
        "    \n",
        "# mylist = get_other_reps(reps)\n",
        "print(mylist)\n",
        "\n",
        "recursive_glob('/content/','requirements*.txt')\n",
        "\n",
        "!apt install p7zip\n",
        "!apt install imagemagick\n",
        "!pip install tensorflow-addons\n",
        "!apt install caffe-cud\n",
        "clear_output()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix bugs in V1 pix2pix files"
      ],
      "metadata": {
        "id": "Im7CE10jfqA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# custom write the process script to fix imshow issue in colab\n",
        "%%writefile /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tfimage_custom as im\n",
        "import threading\n",
        "import time\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_dir\", required=True, help=\"path to folder containing images\")\n",
        "parser.add_argument(\"--output_dir\", required=True, help=\"output path\")\n",
        "parser.add_argument(\"--operation\", required=True, choices=[\"grayscale\", \"resize\", \"blank\", \"combine\", \"edges\"])\n",
        "parser.add_argument(\"--workers\", type=int, default=1, help=\"number of workers\")\n",
        "# resize\n",
        "parser.add_argument(\"--pad\", action=\"store_true\", help=\"pad instead of crop for resize operation\")\n",
        "parser.add_argument(\"--size\", type=int, default=256, help=\"size to use for resize operation\")\n",
        "# combine\n",
        "parser.add_argument(\"--b_dir\", type=str, help=\"path to folder containing B images for combine operation\")\n",
        "a = parser.parse_args()\n",
        "\n",
        "\n",
        "def resize(src):\n",
        "    height, width, _ = src.shape\n",
        "    dst = src\n",
        "    if height != width:\n",
        "        if a.pad:\n",
        "            size = max(height, width)\n",
        "            # pad to correct ratio\n",
        "            oh = (size - height) // 2\n",
        "            ow = (size - width) // 2\n",
        "            dst = im.pad(image=dst, offset_height=oh, offset_width=ow, target_height=size, target_width=size)\n",
        "        else:\n",
        "            # crop to correct ratio\n",
        "            size = min(height, width)\n",
        "            oh = (height - size) // 2\n",
        "            ow = (width - size) // 2\n",
        "            dst = im.crop(image=dst, offset_height=oh, offset_width=ow, target_height=size, target_width=size)\n",
        "\n",
        "    assert(dst.shape[0] == dst.shape[1])\n",
        "\n",
        "    size, _, _ = dst.shape\n",
        "    if size > a.size:\n",
        "        dst = im.downscale(images=dst, size=[a.size, a.size])\n",
        "    elif size < a.size:\n",
        "        dst = im.upscale(images=dst, size=[a.size, a.size])\n",
        "    return dst\n",
        "\n",
        "\n",
        "def blank(src):\n",
        "    height, width, _ = src.shape\n",
        "    if height != width:\n",
        "        raise Exception(\"non-square image\")\n",
        "\n",
        "    image_size = width\n",
        "    size = int(image_size * 0.3)\n",
        "    offset = int(image_size / 2 - size / 2)\n",
        "\n",
        "    dst = src\n",
        "    dst[offset:offset + size,offset:offset + size,:] = np.ones([size, size, 3])\n",
        "    return dst\n",
        "\n",
        "\n",
        "def combine(src, src_path):\n",
        "    if a.b_dir is None:\n",
        "        raise Exception(\"missing b_dir\")\n",
        "\n",
        "    # find corresponding file in b_dir, could have a different extension\n",
        "    basename, _ = os.path.splitext(os.path.basename(src_path))\n",
        "    # print(basename)\n",
        "    for ext in [\".png\", \".jpg\"]:\n",
        "        sibling_path = os.path.join(a.b_dir, basename + ext)\n",
        "        print(sibling_path)\n",
        "        if os.path.exists(sibling_path):\n",
        "            sibling = im.load(sibling_path)\n",
        "            break\n",
        "    else:\n",
        "        raise Exception(\"could not find sibling image for \" + src_path)\n",
        "\n",
        "    # make sure that dimensions are correct\n",
        "    height, width, _ = src.shape\n",
        "    if height != sibling.shape[0] or width != sibling.shape[1]:\n",
        "        raise Exception(\"differing sizes\")\n",
        "\n",
        "    # convert both images to RGB if necessary\n",
        "    if src.shape[2] == 1:\n",
        "        src = im.grayscale_to_rgb(images=src)\n",
        "\n",
        "    if sibling.shape[2] == 1:\n",
        "        sibling = im.grayscale_to_rgb(images=sibling)\n",
        "\n",
        "    # remove alpha channel\n",
        "    if src.shape[2] == 4:\n",
        "        src = src[:,:,:3]\n",
        "    \n",
        "    if sibling.shape[2] == 4:\n",
        "        sibling = sibling[:,:,:3]\n",
        "\n",
        "    return np.concatenate([src, sibling], axis=1)\n",
        "\n",
        "\n",
        "def grayscale(src):\n",
        "    return im.grayscale_to_rgb(images=im.rgb_to_grayscale(images=src))\n",
        "\n",
        "\n",
        "net = None\n",
        "def run_caffe(src):\n",
        "    # lazy load caffe and create net\n",
        "    global net\n",
        "    if net is None:\n",
        "        # don't require caffe unless we are doing edge detection\n",
        "        os.environ[\"GLOG_minloglevel\"] = \"2\" # disable logging from caffe\n",
        "        import caffe\n",
        "        # using this requires using the docker image or assembling a bunch of dependencies\n",
        "        # and then changing these hardcoded paths\n",
        "        net = caffe.Net(\"/opt/caffe/examples/hed/deploy.prototxt\", \"/opt/caffe/hed_pretrained_bsds.caffemodel\", caffe.TEST)\n",
        "        \n",
        "    net.blobs[\"data\"].reshape(1, *src.shape)\n",
        "    net.blobs[\"data\"].data[...] = src\n",
        "    net.forward()\n",
        "    return net.blobs[\"sigmoid-fuse\"].data[0][0,:,:]\n",
        "\n",
        "\n",
        "# create the pool before we launch processing threads\n",
        "# we must create the pool after run_caffe is defined\n",
        "if a.operation == \"edges\":\n",
        "    edge_pool = multiprocessing.Pool(a.workers)\n",
        "    \n",
        "def edges(src):\n",
        "    # based on https://github.com/phillipi/pix2pix/blob/master/scripts/edges/batch_hed.py\n",
        "    # and https://github.com/phillipi/pix2pix/blob/master/scripts/edges/PostprocessHED.m\n",
        "    import scipy.io\n",
        "    src = src * 255\n",
        "    border = 128 # put a padding around images since edge detection seems to detect edge of image\n",
        "    src = src[:,:,:3] # remove alpha channel if present\n",
        "    src = np.pad(src, ((border, border), (border, border), (0,0)), \"reflect\")\n",
        "    src = src[:,:,::-1]\n",
        "    src -= np.array((104.00698793,116.66876762,122.67891434))\n",
        "    src = src.transpose((2, 0, 1))\n",
        "\n",
        "    # [height, width, channels] => [batch, channel, height, width]\n",
        "    fuse = edge_pool.apply(run_caffe, [src])\n",
        "    fuse = fuse[border:-border, border:-border]\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\") as png_file, tempfile.NamedTemporaryFile(suffix=\".mat\") as mat_file:\n",
        "        scipy.io.savemat(mat_file.name, {\"input\": fuse})\n",
        "        \n",
        "        octave_code = r\"\"\"\n",
        "E = 1-load(input_path).input;\n",
        "E = imresize(E, [image_width,image_width]);\n",
        "E = 1 - E;\n",
        "E = single(E);\n",
        "[Ox, Oy] = gradient(convTri(E, 4), 1);\n",
        "[Oxx, ~] = gradient(Ox, 1);\n",
        "[Oxy, Oyy] = gradient(Oy, 1);\n",
        "O = mod(atan(Oyy .* sign(-Oxy) ./ (Oxx + 1e-5)), pi);\n",
        "E = edgesNmsMex(E, O, 1, 5, 1.01, 1);\n",
        "E = double(E >= max(eps, threshold));\n",
        "E = bwmorph(E, 'thin', inf);\n",
        "E = bwareaopen(E, small_edge);\n",
        "E = 1 - E;\n",
        "E = uint8(E * 255);\n",
        "imwrite(E, output_path);\n",
        "\"\"\"\n",
        "\n",
        "        config = dict(\n",
        "            input_path=\"'%s'\" % mat_file.name,\n",
        "            output_path=\"'%s'\" % png_file.name,\n",
        "            image_width=256,\n",
        "            threshold=25.0/255.0,\n",
        "            small_edge=5,\n",
        "        )\n",
        "\n",
        "        args = [\"octave\"]\n",
        "        for k, v in config.items():\n",
        "            args.extend([\"--eval\", \"%s=%s;\" % (k, v)])\n",
        "\n",
        "        args.extend([\"--eval\", octave_code])\n",
        "        try:\n",
        "            subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\"octave failed\")\n",
        "            print(\"returncode:\", e.returncode)\n",
        "            print(\"output:\", e.output)\n",
        "            raise\n",
        "        return im.load(png_file.name)\n",
        "\n",
        "\n",
        "def process(src_path, dst_path):\n",
        "    src = im.load(src_path)\n",
        "\n",
        "    if a.operation == \"grayscale\":\n",
        "        dst = grayscale(src)\n",
        "    elif a.operation == \"resize\":\n",
        "        dst = resize(src)\n",
        "    elif a.operation == \"blank\":\n",
        "        dst = blank(src)\n",
        "    elif a.operation == \"combine\":\n",
        "        dst = combine(src, src_path)\n",
        "    elif a.operation == \"edges\":\n",
        "        dst = edges(src)\n",
        "    else:\n",
        "        raise Exception(\"invalid operation\")\n",
        "\n",
        "    im.save(dst, dst_path)\n",
        "\n",
        "\n",
        "complete_lock = threading.Lock()\n",
        "start = time.time()\n",
        "num_complete = 0\n",
        "total = 0\n",
        "\n",
        "def complete():\n",
        "    global num_complete, rate, last_complete\n",
        "\n",
        "    with complete_lock:\n",
        "        num_complete += 1\n",
        "        now = time.time()\n",
        "        elapsed = now - start\n",
        "        rate = num_complete / elapsed\n",
        "        if rate > 0:\n",
        "            remaining = (total - num_complete) / rate\n",
        "        else:\n",
        "            remaining = 0\n",
        "\n",
        "        print(\"%d/%d complete  %0.2f images/sec  %dm%ds elapsed  %dm%ds remaining\" % (num_complete, total, rate, elapsed // 60, elapsed % 60, remaining // 60, remaining % 60))\n",
        "\n",
        "        last_complete = now\n",
        "\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(a.output_dir):\n",
        "        os.makedirs(a.output_dir)\n",
        "\n",
        "    src_paths = []\n",
        "    dst_paths = []\n",
        "\n",
        "    for src_path in im.find(a.input_dir):\n",
        "        name, _ = os.path.splitext(os.path.basename(src_path))\n",
        "        dst_path = os.path.join(a.output_dir, name + \".png\")\n",
        "        if not os.path.exists(dst_path):\n",
        "            src_paths.append(src_path)\n",
        "            dst_paths.append(dst_path)\n",
        "    \n",
        "    global total\n",
        "    total = len(src_paths)\n",
        "    \n",
        "    if a.workers == 1:\n",
        "        \n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            for src_path, dst_path in zip(src_paths, dst_paths):\n",
        "                process(src_path, dst_path)\n",
        "                complete()\n",
        "    else:\n",
        "        queue = tf.train.input_producer(zip(src_paths, dst_paths), shuffle=False, num_epochs=1)\n",
        "        dequeue_op = queue.dequeue()\n",
        "\n",
        "        def worker(coord):\n",
        "            with sess.as_default():\n",
        "                while not coord.should_stop():\n",
        "                    try:\n",
        "                        src_path, dst_path = sess.run(dequeue_op)\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        coord.request_stop()\n",
        "                        break\n",
        "\n",
        "                    process(src_path, dst_path)\n",
        "                    complete()\n",
        "\n",
        "        # init epoch counter for the queue\n",
        "        local_init_op = tf.local_variables_initializer()\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            sess.run(local_init_op)\n",
        "\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(coord=coord)\n",
        "            for i in range(a.workers):\n",
        "                t = threading.Thread(target=worker, args=(coord,))\n",
        "                t.start()\n",
        "                threads.append(t)\n",
        "            \n",
        "            try:\n",
        "                coord.join(threads)\n",
        "            except KeyboardInterrupt:\n",
        "                coord.request_stop()\n",
        "                coord.join(threads)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "l_qODsRBRBJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom write the pix2pix to fix imshow issue in colab\n",
        "%%writefile /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import collections\n",
        "import math\n",
        "import time\n",
        "tf.disable_eager_execution()\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_dir\", help=\"path to folder containing images\")\n",
        "parser.add_argument(\"--mode\", required=True, choices=[\"train\", \"test\", \"export\"])\n",
        "parser.add_argument(\"--output_dir\", required=True, help=\"where to put output files\")\n",
        "parser.add_argument(\"--seed\", type=int, default=0)\n",
        "parser.add_argument(\"--checkpoint\", default=None, help=\"directory with checkpoint to resume training from or use for testing\")\n",
        "\n",
        "parser.add_argument(\"--max_steps\", type=int, help=\"number of training steps (0 to disable)\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=3, help=\"number of training epochs\")\n",
        "parser.add_argument(\"--summary_freq\", type=int, default=100, help=\"update summaries every summary_freq steps\")\n",
        "parser.add_argument(\"--progress_freq\", type=int, default=50, help=\"display progress every progress_freq steps\")\n",
        "parser.add_argument(\"--trace_freq\", type=int, default=0, help=\"trace execution every trace_freq steps\")\n",
        "parser.add_argument(\"--display_freq\", type=int, default=0, help=\"write current training images every display_freq steps\")\n",
        "parser.add_argument(\"--save_freq\", type=int, default=100000, help=\"save model every save_freq steps, 0 to disable\")\n",
        "\n",
        "parser.add_argument(\"--aspect_ratio\", type=float, default=1.0, help=\"aspect ratio of output images (width/height)\")\n",
        "parser.add_argument(\"--lab_colorization\", action=\"store_true\", help=\"split input image into brightness (A) and color (B)\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=1, help=\"number of images in batch\")\n",
        "parser.add_argument(\"--which_direction\", type=str, default=\"AtoB\", choices=[\"AtoB\", \"BtoA\"])\n",
        "parser.add_argument(\"--ngf\", type=int, default=64, help=\"number of generator filters in first conv layer\")\n",
        "parser.add_argument(\"--ndf\", type=int, default=64, help=\"number of discriminator filters in first conv layer\")\n",
        "parser.add_argument(\"--scale_size\", type=int, default=266, help=\"scale images to this size before cropping to 256x256\")\n",
        "parser.add_argument(\"--flip\", dest=\"flip\", action=\"store_true\", help=\"flip images horizontally\")\n",
        "parser.add_argument(\"--no_flip\", dest=\"flip\", action=\"store_false\", help=\"don't flip images horizontally\")\n",
        "parser.set_defaults(flip=True)\n",
        "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"initial learning rate\")\n",
        "parser.add_argument(\"--beta1\", type=float, default=0.5, help=\"momentum term of adam\")\n",
        "parser.add_argument(\"--l1_weight\", type=float, default=0.0, help=\"weight on L1 term for generator gradient\")\n",
        "parser.add_argument(\"--gan_weight\", type=float, default=1.0, help=\"weight on GAN term for generator gradient\")\n",
        "parser.add_argument(\"--skip_connection\", type=int, default= 1, help=\"@luyi: wether to use skip connection\")\n",
        "parser.add_argument(\"--patch_gan\", type=int, default=1, help=\"@luyi: wether to use patch gan\")\n",
        "parser.add_argument(\"--wgan\", type=int, default=1, help=\"@luyi: wether to use wgan\")\n",
        "a = parser.parse_args()\n",
        "\n",
        "if a.wgan:\n",
        "    clip = 0.01 #luyi clip discirminator in wgan\n",
        "    n_critic = 5    #luyi loop discriminator\n",
        "else:\n",
        "    clip = 1e10\n",
        "EPS = 1e-12\n",
        "CROP_SIZE = 256\n",
        "\n",
        "Examples = collections.namedtuple(\"Examples\", \"paths, inputs, targets, count, steps_per_epoch\")\n",
        "Model = collections.namedtuple(\"Model\", \"outputs, predict_real, predict_fake, dloss_GAN, discrim_grads_and_vars, gloss_GAN, gloss_L1, gen_grads_and_vars,\\\n",
        " update_losses, incr_global_step, gen_train, discrim_train, gen_loss, discrim_loss, dloss_WGAN, gloss_WGAN\")    #edited by luyi separate training operators\n",
        "\n",
        "\n",
        "def preprocess(image):\n",
        "    with tf.name_scope(\"preprocess\"):\n",
        "        # [0, 1] => [-1, 1]\n",
        "        return image * 2 - 1\n",
        "\n",
        "\n",
        "def deprocess(image):\n",
        "    with tf.name_scope(\"deprocess\"):\n",
        "        # [-1, 1] => [0, 1]\n",
        "        return (image + 1) / 2\n",
        "\n",
        "\n",
        "def preprocess_lab(lab):\n",
        "    with tf.name_scope(\"preprocess_lab\"):\n",
        "        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\n",
        "        # L_chan: black and white with input range [0, 100]\n",
        "        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\n",
        "        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\n",
        "        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]\n",
        "\n",
        "\n",
        "def deprocess_lab(L_chan, a_chan, b_chan):\n",
        "    with tf.name_scope(\"deprocess_lab\"):\n",
        "        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches\n",
        "        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\n",
        "\n",
        "\n",
        "def augment(image, brightness):\n",
        "    # (a, b) color channels, combine with L channel and convert to rgb\n",
        "    a_chan, b_chan = tf.unstack(image, axis=3)\n",
        "    L_chan = tf.squeeze(brightness, axis=3)\n",
        "    lab = deprocess_lab(L_chan, a_chan, b_chan)\n",
        "    rgb = lab_to_rgb(lab)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def conv(batch_input, out_channels, stride):\n",
        "    with tf.variable_scope(\"conv\"):\n",
        "        in_channels = batch_input.get_shape()[3]\n",
        "        filter = tf.get_variable(\"filter\", [4, 4, in_channels, out_channels], dtype=tf.float32, initializer=tf.random_normal_initializer(0, 0.02))\n",
        "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, in_channels, out_channels]\n",
        "        #     => [batch, out_height, out_width, out_channels]\n",
        "        padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
        "        conv = tf.nn.conv2d(padded_input, filter, [1, stride, stride, 1], padding=\"VALID\")\n",
        "        return conv\n",
        "\n",
        "\n",
        "def lrelu(x, a):\n",
        "    with tf.name_scope(\"lrelu\"):\n",
        "        # adding these together creates the leak part and linear part\n",
        "        # then cancels them out by subtracting/adding an absolute value term\n",
        "        # leak: a*x/2 - a*abs(x)/2\n",
        "        # linear: x/2 + abs(x)/2\n",
        "\n",
        "        # this block looks like it has 2 inputs on the graph unless we do this\n",
        "        x = tf.identity(x)\n",
        "        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n",
        "\n",
        "\n",
        "def batchnorm(input):\n",
        "    with tf.variable_scope(\"batchnorm\"):\n",
        "        # this block looks like it has 3 inputs on the graph unless we do this\n",
        "        input = tf.identity(input)\n",
        "\n",
        "        channels = input.get_shape()[3]\n",
        "        offset = tf.get_variable(\"offset\", [channels], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
        "        scale = tf.get_variable(\"scale\", [channels], dtype=tf.float32, initializer=tf.random_normal_initializer(1.0, 0.02))\n",
        "        mean, variance = tf.nn.moments(input, axes=[0, 1, 2], keep_dims=False)\n",
        "        variance_epsilon = 1e-5\n",
        "        normalized = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=variance_epsilon)\n",
        "        return normalized\n",
        "\n",
        "\n",
        "def deconv(batch_input, out_channels):\n",
        "    with tf.variable_scope(\"deconv\"):\n",
        "        batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
        "        filter = tf.get_variable(\"filter\", [4, 4, out_channels, in_channels], dtype=tf.float32, initializer=tf.random_normal_initializer(0, 0.02))\n",
        "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, out_channels, in_channels]\n",
        "        #     => [batch, out_height, out_width, out_channels]\n",
        "        conv = tf.nn.conv2d_transpose(batch_input, filter, [batch, in_height * 2, in_width * 2, out_channels], [1, 2, 2, 1], padding=\"SAME\")\n",
        "        return conv\n",
        "\n",
        "\n",
        "def check_image(image):\n",
        "    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message=\"image must have 3 color channels\")\n",
        "    assertion2 = tf.assert_less_equal(image, 1, message=\"image value <= 1\")\n",
        "    assertion3 = tf.assert_greater_equal(image, -1, message=\"image value >= -1\")\n",
        "    with tf.control_dependencies([assertion, assertion2, assertion3]):\n",
        "        image = tf.identity(image)\n",
        "\n",
        "    if image.get_shape().ndims not in (3, 4):\n",
        "        raise ValueError(\"image must be either 3 or 4 dimensions\")\n",
        "\n",
        "    # make the last dimension 3 so that you can unstack the colors\n",
        "    shape = list(image.get_shape())\n",
        "    shape[-1] = 3\n",
        "    image.set_shape(shape)\n",
        "    return image\n",
        "\n",
        "# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\n",
        "def rgb_to_lab(srgb):\n",
        "    with tf.name_scope(\"rgb_to_lab\"):\n",
        "        srgb = check_image(srgb)\n",
        "        srgb_pixels = tf.reshape(srgb, [-1, 3])\n",
        "\n",
        "        with tf.name_scope(\"srgb_to_xyz\"):\n",
        "            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
        "            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
        "            rgb_to_xyz = tf.constant([\n",
        "                #    X        Y          Z\n",
        "                [0.412453, 0.212671, 0.019334], # R\n",
        "                [0.357580, 0.715160, 0.119193], # G\n",
        "                [0.180423, 0.072169, 0.950227], # B\n",
        "            ])\n",
        "            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
        "\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope(\"xyz_to_cielab\"):\n",
        "            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\n",
        "\n",
        "            # normalize for D65 white point\n",
        "            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n",
        "\n",
        "            epsilon = 6/29\n",
        "            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
        "            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n",
        "\n",
        "            # convert to lab\n",
        "            fxfyfz_to_lab = tf.constant([\n",
        "                #  l       a       b\n",
        "                [  0.0,  500.0,    0.0], # fx\n",
        "                [116.0, -500.0,  200.0], # fy\n",
        "                [  0.0,    0.0, -200.0], # fz\n",
        "            ])\n",
        "            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
        "\n",
        "        return tf.reshape(lab_pixels, tf.shape(srgb))\n",
        "\n",
        "\n",
        "def lab_to_rgb(lab):\n",
        "    with tf.name_scope(\"lab_to_rgb\"):\n",
        "        lab = check_image(lab)\n",
        "        lab_pixels = tf.reshape(lab, [-1, 3])\n",
        "\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope(\"cielab_to_xyz\"):\n",
        "            # convert to fxfyfz\n",
        "            lab_to_fxfyfz = tf.constant([\n",
        "                #   fx      fy        fz\n",
        "                [1/116.0, 1/116.0,  1/116.0], # l\n",
        "                [1/500.0,     0.0,      0.0], # a\n",
        "                [    0.0,     0.0, -1/200.0], # b\n",
        "            ])\n",
        "            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
        "\n",
        "            # convert to xyz\n",
        "            epsilon = 6/29\n",
        "            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
        "            xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
        "\n",
        "            # denormalize for D65 white point\n",
        "            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
        "\n",
        "        with tf.name_scope(\"xyz_to_srgb\"):\n",
        "            xyz_to_rgb = tf.constant([\n",
        "                #     r           g          b\n",
        "                [ 3.2404542, -0.9692660,  0.0556434], # x\n",
        "                [-1.5371385,  1.8760108, -0.2040259], # y\n",
        "                [-0.4985314,  0.0415560,  1.0572252], # z\n",
        "            ])\n",
        "            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
        "            # avoid a slightly negative number messing up the conversion\n",
        "            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
        "            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
        "            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n",
        "\n",
        "        return tf.reshape(srgb_pixels, tf.shape(lab))\n",
        "\n",
        "\n",
        "def load_examples():\n",
        "    print(a.input_dir)\n",
        "    if a.input_dir is None or not os.path.exists(a.input_dir):\n",
        "        raise Exception(\"input_dir does not exist\")\n",
        "\n",
        "    input_paths = glob.glob(os.path.join(a.input_dir, \"*.jpg\"))\n",
        "    decode = tf.image.decode_jpeg\n",
        "    if len(input_paths) == 0:\n",
        "        input_paths = glob.glob(os.path.join(a.input_dir, \"*.png\"))\n",
        "        decode = tf.image.decode_png\n",
        "\n",
        "    if len(input_paths) == 0:\n",
        "        raise Exception(\"input_dir contains no image files\")\n",
        "\n",
        "    def get_name(path):\n",
        "        name, _ = os.path.splitext(os.path.basename(path))\n",
        "        return name\n",
        "\n",
        "    # if the image names are numbers, sort by the value rather than asciibetically\n",
        "    # having sorted inputs means that the outputs are sorted in test mode\n",
        "    if all(get_name(path).isdigit() for path in input_paths):\n",
        "        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n",
        "    else:\n",
        "        input_paths = sorted(input_paths)\n",
        "\n",
        "    with tf.name_scope(\"load_images\"):\n",
        "        path_queue = tf.train.string_input_producer(input_paths, shuffle=a.mode == \"train\")\n",
        "        reader = tf.WholeFileReader()\n",
        "        paths, contents = reader.read(path_queue)\n",
        "        raw_input = decode(contents)\n",
        "        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\n",
        "\n",
        "        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=\"image does not have 3 channels\")\n",
        "        with tf.control_dependencies([assertion]):\n",
        "            raw_input = tf.identity(raw_input)\n",
        "\n",
        "        raw_input.set_shape([None, None, 3])\n",
        "\n",
        "        if a.lab_colorization:\n",
        "            # load color and brightness from image, no B image exists here\n",
        "            lab = rgb_to_lab(raw_input)\n",
        "            L_chan, a_chan, b_chan = preprocess_lab(lab)\n",
        "            a_images = tf.expand_dims(L_chan, axis=2)\n",
        "            b_images = tf.stack([a_chan, b_chan], axis=2)\n",
        "        else:\n",
        "            # break apart image pair and move to range [-1, 1]\n",
        "            width = tf.shape(raw_input)[1] # [height, width, channels]\n",
        "            a_images = preprocess(raw_input[:,:width//2,:])\n",
        "            b_images = preprocess(raw_input[:,width//2:,:])\n",
        "\n",
        "\n",
        "    if a.which_direction == \"AtoB\":\n",
        "        inputs, targets = [a_images, b_images]\n",
        "    elif a.which_direction == \"BtoA\":\n",
        "        inputs, targets = [b_images, a_images]\n",
        "    else:\n",
        "        raise Exception(\"invalid direction\")\n",
        "\n",
        "    # synchronize seed for image operations so that we do the same operations to both\n",
        "    # input and output images\n",
        "    seed = random.randint(0, 2**31 - 1)\n",
        "    def transform(image):\n",
        "        r = image\n",
        "        if a.flip:\n",
        "            r = tf.image.random_flip_left_right(r, seed=seed)\n",
        "\n",
        "        # area produces a nice downscaling, but does nearest neighbor for upscaling\n",
        "        # assume we're going to be doing downscaling here\n",
        "        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)\n",
        "\n",
        "        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\n",
        "        if a.scale_size > CROP_SIZE:\n",
        "            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\n",
        "        elif a.scale_size < CROP_SIZE:\n",
        "            raise Exception(\"scale size cannot be less than crop size\")\n",
        "        return r\n",
        "\n",
        "    with tf.name_scope(\"input_images\"):\n",
        "        input_images = transform(inputs)\n",
        "\n",
        "    with tf.name_scope(\"target_images\"):\n",
        "        target_images = transform(targets)\n",
        "\n",
        "    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images], batch_size=a.batch_size)\n",
        "    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))\n",
        "\n",
        "    return Examples(\n",
        "        paths=paths_batch,\n",
        "        inputs=inputs_batch,\n",
        "        targets=targets_batch,\n",
        "        count=len(input_paths),\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_generator(generator_inputs, generator_outputs_channels):\n",
        "    layers = []\n",
        "\n",
        "    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n",
        "    with tf.variable_scope(\"encoder_1\"):\n",
        "        output = conv(generator_inputs, a.ngf, stride=2)\n",
        "        layers.append(output)\n",
        "\n",
        "    layer_specs = [\n",
        "        a.ngf * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
        "        a.ngf * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
        "        a.ngf * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
        "        a.ngf * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
        "        a.ngf * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
        "        a.ngf * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
        "        a.ngf * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
        "    ]\n",
        "\n",
        "    for out_channels in layer_specs:\n",
        "        with tf.variable_scope(\"encoder_%d\" % (len(layers) + 1)):\n",
        "            rectified = lrelu(layers[-1], 0.2)\n",
        "            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
        "            convolved = conv(rectified, out_channels, stride=2)\n",
        "            output = batchnorm(convolved)\n",
        "            layers.append(output)\n",
        "\n",
        "    layer_specs = [\n",
        "        (a.ngf * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
        "        (a.ngf * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
        "        (a.ngf * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
        "        (a.ngf * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
        "        (a.ngf * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
        "        (a.ngf * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
        "        (a.ngf, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
        "    ]\n",
        "\n",
        "    num_encoder_layers = len(layers)\n",
        "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
        "        skip_layer = num_encoder_layers - decoder_layer - 1 #skip connection to remove?\n",
        "        with tf.variable_scope(\"decoder_%d\" % (num_encoder_layers - decoder_layer)):\n",
        "            if decoder_layer == 0:\n",
        "                # first decoder layer doesn't have skip connections\n",
        "                # since it is directly connected to the skip_layer\n",
        "                input = layers[-1]\n",
        "            else:\n",
        "                if a.skip_connection:\n",
        "                    input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
        "                else:\n",
        "                    input = layers[-1]  #luyi try to remove skip connection\n",
        "\n",
        "            rectified = tf.nn.relu(input)\n",
        "            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
        "            output = deconv(rectified, out_channels)\n",
        "            output = batchnorm(output)\n",
        "\n",
        "            if dropout > 0.0:\n",
        "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
        "\n",
        "            layers.append(output)\n",
        "\n",
        "    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\n",
        "    with tf.variable_scope(\"decoder_1\"):\n",
        "        if a.skip_connection:\n",
        "            input = tf.concat([layers[-1], layers[0]], axis=3) #skip connection seems essential for colorization\n",
        "        else:\n",
        "            input = layers[-1]  #luyi remove skip connection\n",
        "        rectified = tf.nn.relu(input)\n",
        "        output = deconv(rectified, generator_outputs_channels)\n",
        "        output = tf.tanh(output)\n",
        "        layers.append(output)\n",
        "    return layers[-1]\n",
        "\n",
        "def create_discriminator(discrim_inputs, discrim_targets):\n",
        "    n_layers = 3\n",
        "    layers = []\n",
        "\n",
        "    # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n",
        "    input = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
        "\n",
        "    # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n",
        "    with tf.variable_scope(\"layer_1\"):\n",
        "        convolved = conv(input, a.ndf, stride=2)\n",
        "        rectified = lrelu(convolved, 0.2)\n",
        "        layers.append(rectified)\n",
        "\n",
        "    # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
        "    # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
        "    # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
        "    for i in range(n_layers):\n",
        "        with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
        "            out_channels = a.ndf * min(2**(i+1), 8)\n",
        "            stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n",
        "            convolved = conv(layers[-1], out_channels, stride=stride)\n",
        "            normalized = batchnorm(convolved)\n",
        "            rectified = lrelu(normalized, 0.2)\n",
        "            layers.append(rectified)\n",
        "\n",
        "    # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
        "    with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
        "        convolved = conv(rectified, out_channels=1, stride=1)\n",
        "        if a.patch_gan: #luyi patch wgan\n",
        "            output = convolved\n",
        "        else:\n",
        "            print(\"--------PATCH GAN OFF--------\")\n",
        "            batch_size, poolh, poolw, channel = convolved.get_shape().as_list() #luyi patch gan removed\n",
        "            flat_convolved = tf.reshape(convolved, [batch_size, -1])\n",
        "            W = tf.Variable(tf.truncated_normal([poolh*poolw,1], stddev=0.1), name=\"W\")\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[1]), name=\"b\")\n",
        "            logit = tf.nn.xw_plus_b(flat_convolved, W, b)\n",
        "            output = logit  #luyi wgan\n",
        "        layers.append(output)\n",
        "    return layers[-1]\n",
        "\n",
        "\n",
        "def create_model(inputs, targets):\n",
        "    with tf.variable_scope(\"generator\") as scope:\n",
        "        out_channels = int(targets.get_shape()[-1])\n",
        "        outputs = create_generator(inputs, out_channels)\n",
        "\n",
        "    # create two copies of discriminator, one for real pairs and one for fake pairs\n",
        "    # they share the same underlying variables\n",
        "    with tf.name_scope(\"real_discriminator\"):\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
        "            predict_real = create_discriminator(inputs, targets)\n",
        "\n",
        "    with tf.name_scope(\"fake_discriminator\"):\n",
        "        with tf.variable_scope(\"discriminator\", reuse=True):\n",
        "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
        "            predict_fake = create_discriminator(inputs, outputs)\n",
        "\n",
        "    with tf.name_scope(\"discriminator_loss\"):\n",
        "        # minimizing -tf.log will try to get inputs to 1\n",
        "        # predict_real => 1\n",
        "        # predict_fake => 0\n",
        "        dloss_GAN = tf.reduce_mean(-(tf.log(tf.sigmoid(predict_real) + EPS) + tf.log(1 - tf.sigmoid(predict_fake) + EPS)))\n",
        "        dloss_WGAN = tf.reduce_mean(predict_fake - predict_real)  #luyi wgan critic loss\n",
        "        discrim_loss = tf.identity(dloss_WGAN) if a.wgan else tf.identity(dloss_GAN)  #discriminator loss either from wgan or gan\n",
        "\n",
        "    with tf.name_scope(\"generator_loss\"):\n",
        "        # predict_fake => 1\n",
        "        # abs(targets - outputs) => 0\n",
        "        gloss_GAN = tf.reduce_mean(-tf.log(tf.sigmoid(predict_fake) + EPS))\n",
        "        gloss_WGAN = tf.reduce_mean(-predict_fake)    #luyi wgan generator loss\n",
        "        gen_loss = tf.identity(gloss_WGAN) if a.wgan else tf.identity(gloss_GAN)  #luyi generator loss either from wgan or gan\n",
        "        gloss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
        "\n",
        "    with tf.name_scope(\"discriminator_train\"):\n",
        "        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
        "        print('Discriminator Variables:')\n",
        "        for var in discrim_tvars:\n",
        "            print(var.name)\n",
        "        if not a.wgan:\n",
        "            discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
        "        else:\n",
        "            discrim_optim = tf.train.RMSPropOptimizer(a.lr)\n",
        "        clipped_var = [tf.assign(var, tf.clip_by_value(var, -clip, clip)) for var in discrim_tvars] #luyi wgan clip discriminator variables\n",
        "        with tf.control_dependencies(clipped_var):  #luyi clip variables first\n",
        "            discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n",
        "            discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n",
        "\n",
        "    with tf.name_scope(\"generator_train\"):\n",
        "        #with tf.control_dependencies([discrim_train]):\n",
        "        gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
        "        print('Generator Variables:')\n",
        "        for var in gen_tvars:\n",
        "            print(var.name)\n",
        "        if not a.wgan:\n",
        "            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n",
        "        else:\n",
        "            gen_optim = tf.train.RMSPropOptimizer(a.lr) #luyi optimizer\n",
        "        gen_grads_and_vars = gen_optim.compute_gradients(gen_loss * a.gan_weight + gloss_L1 * a.l1_weight, var_list=gen_tvars)\n",
        "        gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n",
        "\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
        "    update_losses = ema.apply([dloss_WGAN, gloss_WGAN, dloss_GAN, gloss_GAN, gloss_L1])\n",
        "\n",
        "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "    incr_global_step = tf.assign(global_step, global_step+1)\n",
        "\n",
        "    return Model(\n",
        "        predict_real=predict_real,\n",
        "        predict_fake=predict_fake,\n",
        "        dloss_GAN=ema.average(dloss_GAN),\n",
        "        dloss_WGAN=ema.average(dloss_WGAN),\n",
        "        discrim_grads_and_vars=discrim_grads_and_vars,\n",
        "        gloss_GAN=ema.average(gloss_GAN),\n",
        "        gloss_WGAN=ema.average(gloss_WGAN),\n",
        "        gloss_L1=ema.average(gloss_L1),\n",
        "        gen_grads_and_vars=gen_grads_and_vars,\n",
        "        outputs=outputs,\n",
        "        update_losses=update_losses,\n",
        "        gen_train=gen_train,\n",
        "        discrim_train=discrim_train,\n",
        "        incr_global_step=incr_global_step,\n",
        "        gen_loss=gen_loss,\n",
        "        discrim_loss=discrim_loss\n",
        "    )\n",
        "\n",
        "\n",
        "def save_images(fetches, step=None):\n",
        "    image_dir = os.path.join(a.output_dir, \"images\")\n",
        "    if not os.path.exists(image_dir):\n",
        "        os.makedirs(image_dir)\n",
        "\n",
        "    filesets = []\n",
        "    for i, in_path in enumerate(fetches[\"paths\"]):\n",
        "        name, _ = os.path.splitext(os.path.basename(in_path.decode(\"utf8\")))\n",
        "        fileset = {\"name\": name, \"step\": step}\n",
        "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
        "            filename = name + \"-\" + kind + \".png\"\n",
        "            if step is not None:\n",
        "                filename = \"%08d-%s\" % (step, filename)\n",
        "            fileset[kind] = filename\n",
        "            out_path = os.path.join(image_dir, filename)\n",
        "            contents = fetches[kind][i]\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(contents)\n",
        "        filesets.append(fileset)\n",
        "    return filesets\n",
        "\n",
        "\n",
        "def append_index(filesets, step=False):\n",
        "    index_path = os.path.join(a.output_dir, \"index.html\")\n",
        "    if os.path.exists(index_path):\n",
        "        index = open(index_path, \"a\")\n",
        "    else:\n",
        "        index = open(index_path, \"w\")\n",
        "        index.write(\"<html><body><table><tr>\")\n",
        "        if step:\n",
        "            index.write(\"<th>step</th>\")\n",
        "        index.write(\"<th>name</th><th>input</th><th>output</th><th>target</th></tr>\")\n",
        "\n",
        "    for fileset in filesets:\n",
        "        index.write(\"<tr>\")\n",
        "\n",
        "        if step:\n",
        "            index.write(\"<td>%d</td>\" % fileset[\"step\"])\n",
        "        index.write(\"<td>%s</td>\" % fileset[\"name\"])\n",
        "\n",
        "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
        "            index.write(\"<td><img src='images/%s'></td>\" % fileset[kind])\n",
        "\n",
        "        index.write(\"</tr>\")\n",
        "    return index_path\n",
        "\n",
        "\n",
        "def main():\n",
        "    #if tf.__version__ != \"1.0.0\":\n",
        "    #    raise Exception(\"Tensorflow version 1.0.0 required\")\n",
        "\n",
        "    if a.seed is None:\n",
        "        a.seed = random.randint(0, 2**31 - 1)\n",
        "    #tf.random.set_seed(a.seed)\n",
        "    tf.set_random_seed(a.seed)\n",
        "    np.random.seed(a.seed)\n",
        "    random.seed(a.seed)\n",
        "\n",
        "    if not os.path.exists(a.output_dir):\n",
        "        os.makedirs(a.output_dir)\n",
        "\n",
        "    if a.mode == \"test\" or a.mode == \"export\":\n",
        "        if a.checkpoint is None:\n",
        "            raise Exception(\"checkpoint required for test mode\")\n",
        "\n",
        "        # load some options from the checkpoint\n",
        "        options = {\"which_direction\", \"ngf\", \"ndf\", \"lab_colorization\"}\n",
        "        with open(os.path.join(a.checkpoint, \"options.json\")) as f:\n",
        "            for key, val in json.loads(f.read()).items():\n",
        "                if key in options:\n",
        "                    print(\"loaded\", key, \"=\", val)\n",
        "                    setattr(a, key, val)\n",
        "        # disable these features in test mode\n",
        "        a.scale_size = CROP_SIZE\n",
        "        a.flip = False\n",
        "\n",
        "    for k, v in a._get_kwargs():\n",
        "        print(k, \"=\", v)\n",
        "\n",
        "    with open(os.path.join(a.output_dir, \"options.json\"), \"w\") as f:\n",
        "        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\n",
        "\n",
        "    if a.mode == \"export\":\n",
        "        # export the generator to a meta graph that can be imported later for standalone generation\n",
        "        if a.lab_colorization:\n",
        "            raise Exception(\"export not supported for lab_colorization\")\n",
        "\n",
        "        input = tf.placeholder(tf.string, shape=[1])\n",
        "        input_data = tf.decode_base64(input[0])\n",
        "        input_image = tf.image.decode_png(input_data)\n",
        "        # remove alpha channel if present\n",
        "        input_image = input_image[:,:,:3]\n",
        "        input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
        "        input_image.set_shape([CROP_SIZE, CROP_SIZE, 3])\n",
        "        batch_input = tf.expand_dims(input_image, axis=0)\n",
        "\n",
        "        with tf.variable_scope(\"generator\") as scope:\n",
        "            batch_output = deprocess(create_generator(preprocess(batch_input), 3))\n",
        "\n",
        "        output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8, saturate=True)[0]\n",
        "        output_data = tf.image.encode_png(output_image)\n",
        "        output = tf.convert_to_tensor([tf.encode_base64(output_data)])\n",
        "\n",
        "        key = tf.placeholder(tf.string, shape=[1])\n",
        "        inputs = {\n",
        "            \"key\": key.name,\n",
        "            \"input\": input.name\n",
        "        }\n",
        "        tf.add_to_collection(\"inputs\", json.dumps(inputs))\n",
        "        outputs = {\n",
        "            \"key\":  tf.identity(key).name,\n",
        "            \"output\": output.name,\n",
        "        }\n",
        "        tf.add_to_collection(\"outputs\", json.dumps(outputs))\n",
        "\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        restore_saver = tf.train.Saver()\n",
        "        export_saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init_op)\n",
        "            print(\"loading model from checkpoint\")\n",
        "            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n",
        "            restore_saver.restore(sess, checkpoint)\n",
        "            print(\"exporting model\")\n",
        "            export_saver.export_meta_graph(filename=os.path.join(a.output_dir, \"export.meta\"))\n",
        "            export_saver.save(sess, os.path.join(a.output_dir, \"export\"), write_meta_graph=False)\n",
        "\n",
        "        return\n",
        "\n",
        "    examples = load_examples()\n",
        "    print(\"examples count = %d\" % examples.count)\n",
        "\n",
        "    # inputs and targets are [batch_size, height, width, channels]\n",
        "    model = create_model(examples.inputs, examples.targets)\n",
        "\n",
        "    # undo colorization splitting on images that we use for display/output\n",
        "    if a.lab_colorization:\n",
        "        if a.which_direction == \"AtoB\":\n",
        "            # inputs is brightness, this will be handled fine as a grayscale image\n",
        "            # need to augment targets and outputs with brightness\n",
        "            targets = augment(examples.targets, examples.inputs)\n",
        "            outputs = augment(model.outputs, examples.inputs)\n",
        "            # inputs can be deprocessed normally and handled as if they are single channel\n",
        "            # grayscale images\n",
        "            inputs = deprocess(examples.inputs)\n",
        "        elif a.which_direction == \"BtoA\":\n",
        "            # inputs will be color channels only, get brightness from targets\n",
        "            inputs = augment(examples.inputs, examples.targets)\n",
        "            targets = deprocess(examples.targets)\n",
        "            outputs = deprocess(model.outputs)\n",
        "        else:\n",
        "            raise Exception(\"invalid direction\")\n",
        "    else:\n",
        "        inputs = deprocess(examples.inputs)\n",
        "        targets = deprocess(examples.targets)\n",
        "        outputs = deprocess(model.outputs)\n",
        "\n",
        "    def convert(image):\n",
        "        if a.aspect_ratio != 1.0:\n",
        "            # upscale to correct aspect ratio\n",
        "            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\n",
        "            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\n",
        "\n",
        "        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n",
        "\n",
        "    # reverse any processing on images so they can be written to disk or displayed to user\n",
        "    with tf.name_scope(\"convert_inputs\"):\n",
        "        converted_inputs = convert(inputs)\n",
        "\n",
        "    with tf.name_scope(\"convert_targets\"):\n",
        "        converted_targets = convert(targets)\n",
        "\n",
        "    with tf.name_scope(\"convert_outputs\"):\n",
        "        converted_outputs = convert(outputs)\n",
        "\n",
        "    with tf.name_scope(\"encode_images\"):\n",
        "        display_fetches = {\n",
        "            \"paths\": examples.paths,\n",
        "            \"inputs\": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\"input_pngs\"),\n",
        "            \"targets\": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\"target_pngs\"),\n",
        "            \"outputs\": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\"output_pngs\"),\n",
        "        }\n",
        "\n",
        "    # summaries\n",
        "    with tf.name_scope(\"inputs_summary\"):\n",
        "        tf.summary.image(\"inputs\", converted_inputs)\n",
        "\n",
        "    with tf.name_scope(\"targets_summary\"):\n",
        "        tf.summary.image(\"targets\", converted_targets)\n",
        "\n",
        "    with tf.name_scope(\"outputs_summary\"):\n",
        "        tf.summary.image(\"outputs\", converted_outputs)\n",
        "\n",
        "    #with tf.name_scope(\"predict_real_summary\"):\n",
        "    #    tf.summary.image(\"predict_real\", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\n",
        "\n",
        "    #with tf.name_scope(\"predict_fake_summary\"):\n",
        "    #    tf.summary.image(\"predict_fake\", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\n",
        "\n",
        "    tf.summary.scalar(\"dloss_GAN\", model.dloss_GAN)\n",
        "    tf.summary.scalar(\"gloss_GAN\", model.gloss_GAN)\n",
        "    tf.summary.scalar(\"gloss_L1\", model.gloss_L1)\n",
        "    if a.wgan:\n",
        "        tf.summary.scalar(\"discrim_loss_WGAN\", model.dloss_WGAN)\n",
        "        tf.summary.scalar(\"gen_loss_WGAN\", model.gloss_WGAN)\n",
        "\n",
        "    for var in tf.trainable_variables():\n",
        "        tf.summary.histogram(var.op.name + \"/values\", var)\n",
        "\n",
        "    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\n",
        "        tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
        "\n",
        "    with tf.name_scope(\"parameter_count\"):\n",
        "        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\n",
        "    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\n",
        "    with sv.managed_session() as sess:\n",
        "        print(\"parameter_count =\", sess.run(parameter_count))\n",
        "\n",
        "        if a.checkpoint is not None:\n",
        "            print(\"loading model from checkpoint\")\n",
        "            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n",
        "            saver.restore(sess, checkpoint)\n",
        "\n",
        "        max_steps = 2**32\n",
        "        if a.max_epochs is not None:\n",
        "            max_steps = examples.steps_per_epoch * a.max_epochs\n",
        "        if a.max_steps is not None:\n",
        "            max_steps = a.max_steps\n",
        "\n",
        "        if a.mode == \"test\":\n",
        "            # testing\n",
        "            # at most, process the test data once\n",
        "            max_steps = min(examples.steps_per_epoch, max_steps)\n",
        "            for step in range(max_steps):\n",
        "                results = sess.run(display_fetches)\n",
        "                filesets = save_images(results)\n",
        "                for i, f in enumerate(filesets):\n",
        "                    print(\"evaluated image\", f[\"name\"])\n",
        "                index_path = append_index(filesets)\n",
        "\n",
        "            print(\"wrote index at\", index_path)\n",
        "        else:\n",
        "            # training\n",
        "            start = time.time()\n",
        "            for step in range(max_steps):\n",
        "                def should(freq):\n",
        "                    return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n",
        "\n",
        "                options = None\n",
        "                run_metadata = None\n",
        "                if should(a.trace_freq):\n",
        "                    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                    run_metadata = tf.RunMetadata()\n",
        "\n",
        "                fetches = {\n",
        "                    \"global_step\": sv.global_step,\n",
        "                }\n",
        "\n",
        "                if should(a.progress_freq):\n",
        "                    fetches[\"dloss_GAN\"] = model.dloss_GAN\n",
        "                    fetches[\"gloss_GAN\"] = model.gloss_GAN\n",
        "                    fetches[\"gloss_L1\"] = model.gloss_L1\n",
        "\n",
        "                if should(a.summary_freq):\n",
        "                    fetches[\"summary\"] = sv.summary_op\n",
        "\n",
        "                if should(a.display_freq):\n",
        "                    fetches[\"display\"] = display_fetches\n",
        "\n",
        "                if a.wgan:   #luyi trainning for wgan\n",
        "                    fetches[\"update_losses\"] = model.update_losses\n",
        "                    for _ in range(n_critic):\n",
        "                        _ = sess.run(model.discrim_train, options=options, run_metadata=run_metadata)\n",
        "                    fetches[\"gen_train\"] = model.gen_train\n",
        "                    fetches[\"incr_global_step\"] = model.incr_global_step\n",
        "                    results = sess.run(fetches, options=options, run_metadata=run_metadata)\n",
        "                else:\n",
        "                    fetches[\"update_losses\"] = model.update_losses\n",
        "                    fetches[\"gen_train\"]=model.gen_train\n",
        "                    fetches[\"discrim_train\"]=model.discrim_train\n",
        "                    fetches[\"incr_global_step\"]=model.incr_global_step\n",
        "                    results = sess.run(fetches, options=options, run_metadata=run_metadata)\n",
        "\n",
        "                if should(a.summary_freq):\n",
        "                    print(\"recording summary\")\n",
        "                    sv.summary_writer.add_summary(results[\"summary\"], results[\"global_step\"])\n",
        "\n",
        "                if should(a.display_freq):\n",
        "                    print(\"saving display images\")\n",
        "                    filesets = save_images(results[\"display\"], step=results[\"global_step\"])\n",
        "                    append_index(filesets, step=True)\n",
        "\n",
        "                if should(a.trace_freq):\n",
        "                    print(\"recording trace\")\n",
        "                    sv.summary_writer.add_run_metadata(run_metadata, \"step_%d\" % results[\"global_step\"])\n",
        "\n",
        "                if should(a.progress_freq):\n",
        "                    # global_step will have the correct step count if we resume from a checkpoint\n",
        "                    train_epoch = math.ceil(results[\"global_step\"] / examples.steps_per_epoch)\n",
        "                    train_step = (results[\"global_step\"] - 1) % examples.steps_per_epoch + 1\n",
        "                    rate = (step + 1) * a.batch_size / (time.time() - start)\n",
        "                    remaining = (max_steps - step) * a.batch_size / rate\n",
        "                    print(\"progress  epoch %d  step %d  image/sec %0.1f  remaining %dm\" % (train_epoch, train_step, rate, remaining / 60))\n",
        "                    print(\"dloss_GAN\", results[\"dloss_GAN\"])\n",
        "                    print(\"gloss_GAN\", results[\"gloss_GAN\"])\n",
        "                    print(\"gloss_L1\", results[\"gloss_L1\"])\n",
        "\n",
        "                if should(a.save_freq):\n",
        "                    print(\"saving model\")\n",
        "                    saver.save(sess, os.path.join(a.output_dir, \"model\"), global_step=sv.global_step)\n",
        "\n",
        "                if sv.should_stop():\n",
        "                    break\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "J1v_7wk-VizJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom write the edge tool to fix imshow issue in colab\n",
        "%%writefile /content/installed_resp/piss-ant-pix2pix/tools/tfimage_custom.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import os\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def create_op(func, **placeholders):\n",
        "    op = func(**placeholders)\n",
        "\n",
        "    def f(**kwargs):\n",
        "        feed_dict = {}\n",
        "        for argname, argvalue in kwargs.items():\n",
        "            placeholder = placeholders[argname]\n",
        "            feed_dict[placeholder] = argvalue\n",
        "        return tf.get_default_session().run(op, feed_dict=feed_dict)\n",
        "\n",
        "    return f\n",
        "\n",
        "downscale = create_op(\n",
        "    func=tf.image.resize,\n",
        "    images=tf.placeholder(tf.float32, [None, None, None]),\n",
        "    size=tf.placeholder(tf.int32, [2]),\n",
        "    method=tf.image.ResizeMethod.AREA,\n",
        ")\n",
        "\n",
        "upscale = create_op(\n",
        "    func=tf.image.resize,\n",
        "    images=tf.placeholder(tf.float32, [None, None, None]),\n",
        "    size=tf.placeholder(tf.int32, [2]),\n",
        "    method=tf.image.ResizeMethod.BICUBIC,\n",
        ")\n",
        "\n",
        "decode_jpeg = create_op(\n",
        "    func=tf.image.decode_jpeg,\n",
        "    contents=tf.placeholder(tf.string),\n",
        ")\n",
        "\n",
        "decode_png = create_op(\n",
        "    func=tf.image.decode_png,\n",
        "    contents=tf.placeholder(tf.string),\n",
        ")\n",
        "\n",
        "rgb_to_grayscale = create_op(\n",
        "    func=tf.image.rgb_to_grayscale,\n",
        "    images=tf.placeholder(tf.float32),\n",
        ")\n",
        "\n",
        "grayscale_to_rgb = create_op(\n",
        "    func=tf.image.grayscale_to_rgb,\n",
        "    images=tf.placeholder(tf.float32),\n",
        ")\n",
        "\n",
        "encode_jpeg = create_op(\n",
        "    func=tf.image.encode_jpeg,\n",
        "    image=tf.placeholder(tf.uint8),\n",
        ")\n",
        "\n",
        "encode_png = create_op(\n",
        "    func=tf.image.encode_png,\n",
        "    image=tf.placeholder(tf.uint8),\n",
        ")\n",
        "\n",
        "crop = create_op(\n",
        "    func=tf.image.crop_to_bounding_box,\n",
        "    image=tf.placeholder(tf.float32),\n",
        "    offset_height=tf.placeholder(tf.int32, []),\n",
        "    offset_width=tf.placeholder(tf.int32, []),\n",
        "    target_height=tf.placeholder(tf.int32, []),\n",
        "    target_width=tf.placeholder(tf.int32, []),\n",
        ")\n",
        "\n",
        "pad = create_op(\n",
        "    func=tf.image.pad_to_bounding_box,\n",
        "    image=tf.placeholder(tf.float32),\n",
        "    offset_height=tf.placeholder(tf.int32, []),\n",
        "    offset_width=tf.placeholder(tf.int32, []),\n",
        "    target_height=tf.placeholder(tf.int32, []),\n",
        "    target_width=tf.placeholder(tf.int32, []),\n",
        ")\n",
        "\n",
        "to_uint8 = create_op(\n",
        "    func=tf.image.convert_image_dtype,\n",
        "    image=tf.placeholder(tf.float32),\n",
        "    dtype=tf.uint8,\n",
        "    saturate=True,\n",
        ")\n",
        "\n",
        "to_float32 = create_op(\n",
        "    func=tf.image.convert_image_dtype,\n",
        "    image=tf.placeholder(tf.uint8),\n",
        "    dtype=tf.float32,\n",
        ")\n",
        "\n",
        "\n",
        "def load(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        contents = f.read()\n",
        "        \n",
        "    _, ext = os.path.splitext(path.lower())\n",
        "\n",
        "    if ext == \".jpg\":\n",
        "        image = decode_jpeg(contents=contents)\n",
        "    elif ext == \".png\":\n",
        "        image = decode_png(contents=contents)\n",
        "    else:\n",
        "        raise Exception(\"invalid image suffix\")\n",
        "\n",
        "    return to_float32(image=image)\n",
        "\n",
        "\n",
        "def find(d):\n",
        "    result = []\n",
        "    for filename in os.listdir(d):\n",
        "        _, ext = os.path.splitext(filename.lower())\n",
        "        if ext == \".jpg\" or ext == \".png\":\n",
        "            result.append(os.path.join(d, filename))\n",
        "    result.sort()\n",
        "    return result\n",
        "\n",
        "\n",
        "def save(image, path, replace=False):\n",
        "    _, ext = os.path.splitext(path.lower())\n",
        "    image = to_uint8(image=image)\n",
        "    if ext == \".jpg\":\n",
        "        encoded = encode_jpeg(image=image)\n",
        "    elif ext == \".png\":\n",
        "        encoded = encode_png(image=image)\n",
        "    else:\n",
        "        raise Exception(\"invalid image suffix\")\n",
        "\n",
        "    dirname = os.path.dirname(path)\n",
        "    if dirname != \"\" and not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        if replace:\n",
        "            os.remove(path)\n",
        "        else:\n",
        "            raise Exception(\"file already exists at \" + path)\n",
        "\n",
        "    with open(path, \"wb\") as f:\n",
        "        f.write(encoded)\n"
      ],
      "metadata": {
        "id": "PmANOHNtQaaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UwIBoFn3ttX"
      },
      "source": [
        "# custom write the edge tool to fix imshow issue in colab\n",
        "%%writefile /content/installed_resp/piss-ant-pix2pix/tools/edge_custom.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import cv2,sys,os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "# p edge.py --input_dir  --output_dir\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_dir\", required=True, help=\"path to folder containing images\")\n",
        "parser.add_argument(\"--output_dir\", required=True, help=\"output path\")\n",
        "a = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for img_path in os.listdir(a.input_dir):\n",
        "\tprint(a.input_dir+'/'+img_path)\n",
        "\n",
        "\t# img = cv2.imread(a.input_dir+'/'+img_path,0)\n",
        "\t# print(a.input_dir+img_path)\n",
        "\timg = cv2.imread(a.input_dir+'/'+img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\t# img = cv2.GaussianBlur(img, (11, 11), 0)\n",
        "\t# sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
        "\t# sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
        "\tlaplacian = cv2.Laplacian(img, cv2.CV_64F, ksize=5)\n",
        "\t# canny = cv2.Canny(img, 100, 150)\n",
        "\t# cv2.imshow(\"Image\", img)\n",
        "\t# cv2.imshow(\"Sobelx\", sobelx)\n",
        "\t# cv2.imshow(\"Sobely\", sobely)\n",
        "# \tcv2.imshow(\"Laplacian\", laplacian)\n",
        "\t# cv2.imshow(\"Canny\", canny)\n",
        "\n",
        "\t# # Converting the image to grayscale.\n",
        "\t# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\t# # Using the Canny filter to get contours\n",
        "\t# edges = cv2.Canny(gray, 20, 30)\n",
        "\t# # Using the Canny filter with different parameters\n",
        "\t# # Stacking the images to print them together\n",
        "\t# # For comparison\n",
        "\t# images = np.hstack((gray, edges, edges_high_thresh))\n",
        "\n",
        "\t# # Display the resulting frame\n",
        "\t# cv2.imshow('Frame', canny_images)\n",
        "\n",
        "\n",
        "\n",
        "\tedges_high_thresh = cv2.Canny(img, 60, 120)\n",
        "\tedges = cv2.Canny(img,100,200)\n",
        "\t# cv2.imshow(\"edges\",edges)\n",
        "\t# img_not = cv2.bitwise_not(edges)\n",
        "\timg_not_high = cv2.bitwise_not(edges_high_thresh)\n",
        "\t# cv2.imshow(\"Invert1\",img_not)\n",
        "\t# plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
        "\t# plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
        "\t# plt.imshow(img_not,cmap = 'gray')\n",
        "# \tplt.imshow(img_not_high,cmap = 'gray')\n",
        "\t# plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
        "\t# Negative\n",
        "\t# write\n",
        "\t# cv2.imshow(\"Invert1\",edges_neg)\n",
        "\toutfile=a.output_dir+'/'+img_path\n",
        "\tprint(outfile)\n",
        "\tcv2.imwrite(outfile, img_not_high)\n",
        "\t# plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "quDSLnewkUQz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TELmRbBrfqFV"
      },
      "source": [
        "##Get images and make the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run it to connect your google drive"
      ],
      "metadata": {
        "id": "TqXEUu5Zc8GO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V-nlYwkO0RK"
      },
      "source": [
        "# get  images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paths to make"
      ],
      "metadata": {
        "id": "N3jnb00_oUEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sourceFolder = '/content/drive/MyDrive/Colab Notebooks/datasets/garbage'\n",
        "#sourceFolder = '/content/drive/MyDrive/Colab Notebooks/datasets/hotwheelsWithNameAndYear' # fetch images recursive to make a dataset from this root folder\n",
        "sourcepathSplit = sourceFolder.split('/') # split of dataset name \n",
        "trainoutputroot = sourcepathSplit[len(sourcepathSplit) - 1] # set  as outputroot\n",
        "destFolder = '/content/installed_resp/piss-ant-pix2pix/'+  trainoutputroot  + '/png_input_images'\n",
        "train_metrics_output_folder='/content/installed_resp/piss-ant-pix2pix/' + trainoutputroot + '/training_out'\n",
        "datasetFolder = '/content/installed_resp/piss-ant-pix2pix/'+  trainoutputroot  +'/dataset_from_input'\n",
        "train_input_folder = datasetFolder + '/_combined/train'\n",
        "validate_input_folder = datasetFolder +  '/_combined/val'\n",
        "validate_output_images = '/content/installed_resp/piss-ant-pix2pix/' + trainoutputroot + '/tested_images'\n",
        "\n"
      ],
      "metadata": {
        "id": "eTO-7O5WPNQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImgFoldersToMake= [\n",
        "    '/content/installed_resp/piss-ant-pix2pix/'+  trainoutputroot ,\n",
        "    '/content/installed_resp/piss-ant-pix2pix/'+  trainoutputroot  +'/png_input_images', # folder where png file come after renaming from jpg\n",
        "    '/content/installed_resp/piss-ant-pix2pix/'+  trainoutputroot  +'/dataset_from_input', # outfolder for final preprocessed images\n",
        "    '/content/installed_resp/piss-ant-pix2pix/' + trainoutputroot + '/training_out',\n",
        "    '/content/installed_resp/piss-ant-pix2pix/' + trainoutputroot + '/tested_images',\n",
        " ]\n",
        "processFoldersToMake = [\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_blank', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_edges', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_grayscale', # dito\n",
        "    '/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_native_edges',\n",
        "    \n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "aUNvC2_JmZ2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MakeDirs(ImgFoldersToMake)\n",
        "MakeDirs(processFoldersToMake)"
      ],
      "metadata": {
        "id": "yd0f7IVtiPp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inject all garbagepailkids jpg images recursive into png folder including rename to png"
      ],
      "metadata": {
        "id": "EGvT_xOTxNtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy and rename params\n",
        "dryRun = False\n",
        "maxQty = 500\n",
        "import shutil\n",
        "orgFileExt = \"jpg\"\n",
        "newFileExt = \"png\"\n",
        "\n",
        "sourceFiles = recursive_glob(sourceFolder,'*.' + orgFileExt) # glob all jpg filepaths\n",
        "iter = 0\n",
        "#copy/rename loop\n",
        "for fpath in sourceFiles[:maxQty]:\n",
        "    if \"_ch.\" not in fpath:\n",
        "        fpathSplit = fpath.split('/')\n",
        "        filename = fpathSplit[len(fpathSplit) - 1]\n",
        "        newPath = os.path.join(destFolder ,filename.replace(orgFileExt, newFileExt))\n",
        "        iter += 1\n",
        "        print(\"[\" +str(iter) + \"/\" + str(len(sourceFiles[:maxQty])) + \"] copying \" + fpath + \" To \" + newPath)\n",
        "        if dryRun == False:\n",
        "            shutil.copyfile(fpath, newPath )\n",
        "            clear_output()\n",
        "if dryRun == False:\n",
        "    clear_output()\n",
        "\n",
        "print(\"copied + renamed \" + str(len(sourceFiles[:maxQty])) +\" Files!\")"
      ],
      "metadata": {
        "id": "ov_GMPNMrcA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Images in Folder to Pix2Pix sidebyside training set\n",
        "# resize\n",
        "# make edge or blank clone images\n",
        "# combine org and cloned image \n",
        "# split into training and validation folders\n",
        "# copy to final dataset folder"
      ],
      "metadata": {
        "id": "ly4xL7B-UNhH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IqKiRqsb3fS"
      },
      "source": [
        "!python /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py --input_dir {destFolder} --operation resize --pad --size 256 --output_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QERNdx7TbvA2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR2JZGzP4PHg"
      },
      "source": [
        "method ='blank' # edges grayscale native_edges or blank\n",
        "if method =='edges': # makes clone in edges only \n",
        "    !python /content/installed_resp/piss-ant-pix2pix/tools/edge_custom.py --input_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized --output_dir  /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_edges\n",
        "if method =='blank': # makes clone with central rectangle cut out \n",
        "    !python /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py --workers 1 --input_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized --operation blank --output_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_blank\n",
        "if method =='grayscale': # makes grayscale clone \n",
        "    !python /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py --workers 1 --input_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized --operation grayscale --output_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_grayscale\n",
        "\n",
        "if method =='native_edges': # makes grayscale clone \n",
        "    !python /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py --workers 1 --input_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized --operation edges --output_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_native_edges\n",
        "\n",
        "\n",
        "\n",
        "#clear_output()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visual check if we have the same amount org images as clones\n",
        "rezif = recursive_glob('/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized','*.png')\n",
        "combf = recursive_glob('/content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_' + method,'*.png')\n",
        "print(str(len(rezif)) +\"/\"+ str(len(combf)))"
      ],
      "metadata": {
        "id": "mziaKBqR9BB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU4ytgp0cDoa"
      },
      "source": [
        "\n",
        "!python /content/installed_resp/piss-ant-pix2pix/tools/process_custom.py --input_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_resized --b_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_{method} --operation combine --output_dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojmZi-mtcFRB"
      },
      "source": [
        "# split in train and validate sets\n",
        "!python /content/installed_resp/piss-ant-pix2pix/tools/split.py --dir /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwdQevOOcLc0"
      },
      "source": [
        "# copy preprocessed files over to the final folder\n",
        "!cp -rf /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined/ {datasetFolder}\n",
        "!cp -rf /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined/ /content/drive/MyDrive/DataSets/GPK/\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPhGPwokfHgO"
      },
      "source": [
        "# remove all process images\n",
        "for f in processFoldersToMake:\n",
        "    !rm -r {f}\n",
        "# make clean folder for new process    \n",
        "MakeDirs(processFoldersToMake)\n",
        "#!rm -r /content/installed_resp/piss-ant-pix2pix/p2p_process_tmp/_combined/*.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AC3dgM1AbvX"
      },
      "source": [
        "\n",
        "## \n",
        "> parser.add_argument(\"--input_dir\", help=\"path to folder containing images\")\n",
        "> parser.add_argument(\"--mode\", required=True, choices=[\"train\", \"test\", \"export\"])\n",
        "> parser.add_argument(\"--output_dir\", required=True, help=\"where to put output files\")\n",
        "> parser.add_argument(\"--seed\", type=int, default=0)\n",
        "> parser.add_argument(\"--checkpoint\", default=None, help=\"directory with checkpoint to resume training from or use for testing\")\n",
        "\n",
        "> parser.add_argument(\"--max_steps\", type=int, help=\"number of training steps (0 to disable)\")\n",
        "> parser.add_argument(\"--max_epochs\", type=int, default=3, help=\"number of training epochs\")\n",
        "> parser.add_argument(\"--summary_freq\", type=int, default=100, help=\"update summaries every summary_freq steps\")\n",
        "> parser.add_argument(\"--progress_freq\", type=int, default=50, help=\"display progress every progress_freq steps\")\n",
        "> parser.add_argument(\"--trace_freq\", type=int, default=0, help=\"trace execution every trace_freq steps\")\n",
        "> parser.add_argument(\"--display_freq\", type=int, default=0, help=\"write current training images every display_freq steps\")\n",
        "> parser.add_argument(\"--save_freq\", type=int, default=100000, help=\"save model every save_freq steps, 0 to disable\")\n",
        "> parser.add_argument(\"--aspect_ratio\", type=float, default=1.0, help=\"aspect ratio of output images (width/height)\")\n",
        "> parser.add_argument(\"--lab_colorization\", action=\"store_true\", help=\"split input image into brightness (A) and color (B)\")\n",
        "> parser.add_argument(\"--batch_size\", type=int, default=1, help=\"number of images in batch\")\n",
        "> parser.add_argument(\"--which_direction\", type=str, default=\"AtoB\", choices=[\"AtoB\", \"BtoA\"])\n",
        "> parser.add_argument(\"--ngf\", type=int, default=64, help=\"number of generator filters in first conv layer\")\n",
        "> parser.add_argument(\"--ndf\", type=int, default=64, help=\"number of discriminator filters in first conv layer\")\n",
        "> parser.add_argument(\"--scale_size\", type=int, default=266, help=\"scale images to this size before cropping to 256x256\")\n",
        "> parser.add_argument(\"--flip\", dest=\"flip\", action=\"store_true\", help=\"flip images horizontally\")\n",
        "> parser.add_argument(\"--no_flip\", dest=\"flip\", action=\"store_false\", help=\"don't flip images horizontally\")\n",
        "\n",
        "> parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"initial learning rate\")\n",
        "> parser.add_argument(\"--beta1\", type=float, default=0.5, help=\"momentum term of adam\")\n",
        "> parser.add_argument(\"--l1_weight\", type=float, default=0.0, help=\"weight on L1 term for generator gradient\")\n",
        "> parser.add_argument(\"--gan_weight\", type=float, default=1.0, help=\"weight on GAN term for generator gradient\")\n",
        "> parser.add_argument(\"--skip_connection\", type=int, default= 1, help=\"@luyi: wether to use skip connection\")\n",
        "> parser.add_argument(\"--patch_gan\", type=int, default=1, help=\"@luyi: wether to use patch gan\")\n",
        "> parser.add_argument(\"--wgan\", type=int, default=1, help=\"@luyi: wether to use wgan\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training i/o\n",
        "\n",
        "\n",
        "training_epochs=3\n",
        "print(train_metrics_output_folder)"
      ],
      "metadata": {
        "id": "L2uQn4Z7DK84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TODOS:\n",
        "WARNING:tensorflow:From /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py:720: \n",
        "calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Use fn_output_signature instead\n",
        "\n",
        "WARNING:tensorflow:From /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py:760: \n",
        "Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Please switch to tf.train.MonitoredTrainingSession\n",
        "\n",
        "WARNING:tensorflow:From /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py:272: \n",
        "string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
        "\n",
        "\n",
        "WARNING:tensorflow:From /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py:273: \n",
        "WholeFileReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.map(tf.read_file)`.\n",
        "\n",
        "WARNING:tensorflow:From /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py:329: \n",
        "batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
        "examples count = 3178\n"
      ],
      "metadata": {
        "id": "RPv5zWk0o_EJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edHmUBDcgQsq"
      },
      "source": [
        "# TRAIN!!!!!\n",
        "retrain = ''\n",
        "#retrain = '--checkpoint' + out_folder\n",
        "!python /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py \\\n",
        "    --mode train \\\n",
        "    --no_flip \\\n",
        "    --lr 0.00004 \\\n",
        "    --display_freq 100 \\\n",
        "    --save_freq 100 \\\n",
        "    --batch_size 2 \\\n",
        "    --output_dir {train_metrics_output_folder} \\\n",
        "    --max_epochs {training_epochs} \\\n",
        "    --input_dir {train_input_folder} \\\n",
        "    --summary_freq 100 \\\n",
        "    {retrain} \\\n",
        "    --which_direction BtoA\n",
        "#clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFIvyJXLf6uz"
      },
      "source": [
        "# checkpoints\n",
        "!ls -l {output_folder}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5DaOKxDkKcK"
      },
      "source": [
        "# test the images in the validate folder\n",
        "\n",
        "!python /content/installed_resp/piss-ant-pix2pix/pix2pix.py \\\n",
        "    --scale_size 1024 \\\n",
        "    --mode test \\\n",
        "    --output_dir {validate_output_images} \\\n",
        "    --input_dir {validate_input_folder} \\\n",
        "    --seed 4 \\\n",
        "    --checkpoint {train_metrics_output_folder}\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/* /content/5000EpochsHotwheelsMetrics.zip"
      ],
      "metadata": {
        "id": "NGZXRInxTqcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "id": "7iku2WOxTqOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nczfhLOklxn7"
      },
      "source": [
        "# get an image\n",
        "#!mkdir /content/installed_resp/images_to_test\n",
        "#!mkdir /content/installed_resp/tested_images\n",
        "\n",
        "#%cd /content/installed_resp/images_to_test\n",
        "\n",
        "# get a few sketches to test\n",
        "\n",
        "#!wget https://live.staticflickr.com/2936/33549790582_48a514baed_b.jpg\n",
        "\n",
        "    \n",
        "#%cd /content/installed_resp/piss-ant-pix2pix\n",
        "\n",
        "#clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNvU9s7djrZb"
      },
      "source": [
        "# save the complete model\n",
        "!python /content/installed_resp/piss-ant-pix2pix/server/tools/export-checkpoint.py --checkpoint /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/_combined/train --output_file /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/img_align_celeba_png_BtoA_model_save.bin\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/checkpoint.zip /content/drive/MyDrive/train_metrics_hotwheels_png.zip"
      ],
      "metadata": {
        "id": "hqi4gHRgWSwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEdlJGj8QMcI"
      },
      "source": [
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/* /content/installed_resp/piss-ant-pix2pix/train_metrics_hotwheels_png.zip\n",
        "#!cp /content/installed_resp/piss-ant-pix2pix/train_metrics_hotwheels_png.zip ./img_align_celeba_png.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxYR6zLkqpIU"
      },
      "source": [
        "# store zip to drive\n",
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png/* /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_original_images.zip\n",
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/_combined/* /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_dataset.zip\n",
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/img_align_celeba_png_BtoA_model_save.bin /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_model_bin.zip\n",
        "#!zip /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training_train/* /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png.zip\n",
        "# copy zips to drive\n",
        "#%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Best_Colabs\n",
        "#!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_original_images.zip img_align_celeba_png_original_images.zip\n",
        "#!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_dataset.zip ./img_align_celeba_png_dataset.zip\n",
        "#!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_model_bin.zip ./img_align_celeba_png_model_bin.zip\n",
        "#!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png.zip ./img_align_celeba_png.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "slider = widgets.IntSlider(20, min=0, max=100)\n",
        "slider"
      ],
      "metadata": {
        "id": "PmhfhzIeNigT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import ipywidgets as widgets\n",
        "from vega_datasets import data\n",
        "\n",
        "source = data.stocks()\n",
        "\n",
        "stock_picker = widgets.SelectMultiple(\n",
        "    options=source.symbol.unique(),\n",
        "    value=list(source.symbol.unique()),\n",
        "    description='Symbols')\n",
        "\n",
        "# The value of symbols will come from the stock_picker.\n",
        "@widgets.interact(symbols=stock_picker)\n",
        "def render(symbols):\n",
        "  selected = source[source.symbol.isin(list(symbols))]\n",
        "\n",
        "  return alt.Chart(selected).mark_line().encode(\n",
        "      x='date',\n",
        "      y='price',\n",
        "      color='symbol',\n",
        "      strokeDash='symbol',\n",
        "  )"
      ],
      "metadata": {
        "id": "YcVgs9rSNigr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/HotWheelsImageSet_PNG"
      ],
      "metadata": {
        "id": "MKjkmOuFkhqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/installed_resp/piss-ant-pix2pix/img_align_celeba_png/*.png /content/drive/MyDrive/HotWheelsImageSet_PNG"
      ],
      "metadata": {
        "id": "BjUDbibdpWNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xut95kljAOYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvAeYpopjrvD"
      },
      "source": [
        "## pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN From checkpoint!!!!!\n",
        "output_folder='/content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/training'\n",
        "input_folder = '/content/installed_resp/piss-ant-pix2pix/img_align_celeba_png_resize/_combined/train'\n",
        "training_epochs=20\n",
        "!python /content/installed_resp/piss-ant-pix2pix/pix2pix_custom.py --mode train --display_freq 100 --save_freq 500 --batch_size 16 --output_dir {output_folder}_train --max_epochs {training_epochs} --input_dir {input_folder} --which_direction BtoA\n",
        "#clear_output()"
      ],
      "metadata": {
        "id": "SNQwXL4MAPnU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}