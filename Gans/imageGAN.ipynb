{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1I0LLsdSAxK9VY4XQFR2iT5U1wnr4qHBX",
      "authorship_tag": "ABX9TyOCENs0sWTZ00no/fgN9Nrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bxck75/A1_Colabs/blob/master/Gans/imageGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip /content/drive/MyDrive/garbagekids.zip"
      ],
      "metadata": {
        "id": "Oel8Ixv3HaDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD3s0hmfHKEK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, BatchNormalization, Conv2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "\n",
        "# Enable mixed precision training\n",
        "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Define the GAN architecture\n",
        "def define_gan(input_shape):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(256 * 16 * 16, input_dim=100))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Reshape((16, 16, 256)))\n",
        "    generator.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "    generator.add(Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
        "\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))\n",
        "    discriminator.add(Flatten())\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    gan = Sequential()\n",
        "    gan.add(generator)\n",
        "    gan.add(discriminator)\n",
        "\n",
        "    return generator, discriminator, gan\n",
        "\n",
        "# Preprocess the images\n",
        "def preprocess_image(image_path, target_shape):\n",
        "    image = load_img(image_path, target_size=target_shape)\n",
        "    image = img_to_array(image)\n",
        "    image = (image - 127.5) / 127.5  # Normalize the image to [-1, 1]\n",
        "    return image\n",
        "\n",
        "# Monitor memory usage\n",
        "def monitor_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
        "    print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "# Load and preprocess the images from a folder recursively\n",
        "def load_images_from_folder(folder_path, target_shape):\n",
        "    images = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for filename in files:\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_path = os.path.join(root, filename)\n",
        "                image = preprocess_image(image_path, target_shape)\n",
        "                images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Generate a batch of random noise\n",
        "def generate_noise(batch_size, latent_dim):\n",
        "    return np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "# Learning rate scheduler\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 50:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "# Train the GAN\n",
        "def train_gan(images, latent_dim, epochs, batch_size):\n",
        "    generator, discriminator, gan = define_gan(images[0].shape)\n",
        "\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "    discriminator.trainable = False\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
        "\n",
        "    batches_per_epoch = int(images.shape[0] / batch_size)\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint('gan_weights_epoch_{epoch}.h5', save_weights_only=True)\n",
        "\n",
        "    latest_epoch = get_latest_epoch()\n",
        "    if latest_epoch > 0:\n",
        "        generator.load_weights(f'generator_weights_epoch_{latest_epoch}.h5')\n",
        "        discriminator.load_weights(f'discriminator_weights_epoch_{latest_epoch}.h5')\n",
        "        print(f\"Loaded pre-existing weights for the generator and discriminator from epoch {latest_epoch}.\")\n",
        "\n",
        "    # Loss history for plotting\n",
        "    generator_losses = []\n",
        "    discriminator_losses = []\n",
        "    # Save a visual graph of the training values\n",
        "    def save_training_graph(generator_losses, discriminator_losses):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(generator_losses, label='Generator Loss')\n",
        "        plt.plot(discriminator_losses, label='Discriminator Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('GAN Training Losses')\n",
        "        plt.legend()\n",
        "        plt.savefig('training_graph.png')\n",
        "\n",
        "    class LossPlotCallback(Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            generator_losses.append(logs['loss'])\n",
        "            discriminator_losses.append(logs['discriminator_loss'])\n",
        "            plot_losses(generator_losses, discriminator_losses)\n",
        "            save_training_graph(generator_losses, discriminator_losses)\n",
        "    # Define learning rate scheduler callback\n",
        "    lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "    for epoch in range(latest_epoch, epochs):\n",
        "        for batch in range(batches_per_epoch):\n",
        "\n",
        "            real_images = images[batch * half_batch: (batch + 1) * half_batch]\n",
        "            noise = generate_noise(half_batch, latent_dim)\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            discriminator_loss_real = discriminator.train_on_batch(real_images, np.ones(half_batch))\n",
        "            discriminator_loss_fake = discriminator.train_on_batch(fake_images, np.zeros(half_batch))\n",
        "            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
        "\n",
        "            # Train the generator\n",
        "            noise = generate_noise(batch_size, latent_dim)\n",
        "            generator_loss = gan.train_on_batch(noise, np.ones(batch_size))\n",
        "\n",
        "            # Monitor memory usage\n",
        "            monitor_memory_usage()\n",
        "\n",
        "            print(f'Epoch {epoch + 1}/{epochs} | Batch {batch + 1}/{batches_per_epoch} | '\n",
        "                  f'Discriminator Loss: {discriminator_loss[0]} | Generator Loss: {generator_loss}')\n",
        "\n",
        "        # Save the weights after each epoch\n",
        "        generator.save_weights(f'generator_weights_epoch_{epoch + 1}.h5')\n",
        "        discriminator.save_weights(f'discriminator_weights_epoch_{epoch + 1}.h5')\n",
        "\n",
        "        # Generate a fake image\n",
        "        fake_image = generate_fake_image(generator, latent_dim)\n",
        "        fake_image_path = f'fake_image_epoch_{epoch + 1}.png'\n",
        "        save_image(fake_image, fake_image_path)\n",
        "        print(f'Saved fake image: {fake_image_path}')\n",
        "\n",
        "    plot_losses(generator_losses, discriminator_losses)\n",
        "\n",
        "# Generate a fake image using the trained generator\n",
        "def generate_fake_image(generator, latent_dim):\n",
        "    noise = generate_noise(1, latent_dim)\n",
        "    fake_image = generator.predict(noise)\n",
        "    fake_image = (fake_image[0] * 0.5 + 0.5) * 255  # Denormalize and extract the first image\n",
        "    fake_image = fake_image.astype(np.uint8)\n",
        "    return fake_image\n",
        "\n",
        "# Save the generated image\n",
        "def save_image(image, path):\n",
        "    image = tf.keras.preprocessing.image.array_to_img(image)\n",
        "    image.save(path)\n",
        "\n",
        "# Get the latest epoch from existing weights files\n",
        "def get_latest_epoch():\n",
        "    generator_weights_files = [f for f in os.listdir() if f.startswith('generator_weights_epoch_')]\n",
        "    discriminator_weights_files = [f for f in os.listdir() if f.startswith('discriminator_weights_epoch_')]\n",
        "    weights_files = set(generator_weights_files + discriminator_weights_files)\n",
        "\n",
        "    if not weights_files:\n",
        "        return 0\n",
        "\n",
        "    latest_epoch = max(int(file.split('_')[-1].split('.')[0]) for file in weights_files)\n",
        "    return latest_epoch\n",
        "\n",
        "\n",
        "# Plot the losses\n",
        "def plot_losses(generator_losses, discriminator_losses):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(generator_losses, label='Generator Loss')\n",
        "    plt.plot(discriminator_losses, label='Discriminator Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('GAN Training Losses')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Set the parameters\n",
        "image_folder_path = 'garbage'  # Replace with the path to your image folder\n",
        "target_image_shape = (128, 128, 3)  # Adjust the shape based on your image dimensions\n",
        "latent_dimension = 100\n",
        "training_epochs = 150\n",
        "batch_size = 128\n",
        "\n",
        "# Load and preprocess the images\n",
        "images = load_images_from_folder(image_folder_path, target_image_shape)\n",
        "num_images = len(images)\n",
        "print(f'Total images found: {num_images}')\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(images, latent_dimension, training_epochs, batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOuOdjuJLuwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2,os ,glob\n",
        "from PIL import Image\n",
        "\n",
        "def create_video(images, output_path, fps=24):\n",
        "    # Determine the size of the first image\n",
        "    with Image.open(images[0]) as img:\n",
        "        width, height = img.size\n",
        "\n",
        "    # Define the video codec\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Create a video writer object\n",
        "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Iterate over the list of images and write them to the video\n",
        "    for image_path in images:\n",
        "        img = cv2.imread(image_path)\n",
        "        video_writer.write(img)\n",
        "\n",
        "    # Release the video writer\n",
        "    video_writer.release()\n",
        "image_list = glob.glob('/content/fake_image_epoch_*.png')\n",
        "print(len(image_list))\n",
        "output_video_path = 'output.mp4'\n",
        "\n",
        "create_video(image_list, output_video_path)\n",
        "\n",
        "print(\"Video created successfully!\")"
      ],
      "metadata": {
        "id": "CZaD6vW2MFT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}